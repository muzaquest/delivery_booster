from __future__ import annotations

from typing import Optional, Dict, Tuple
from datetime import date
import sqlite3
import pandas as pd
import os

from app.report_basic import (
    build_basic_report,
    build_marketing_report,
    build_quality_report,
)
from etl.data_loader import get_engine
import numpy as np
import re
from ml.inference import load_artifacts, _resolve_preprocessed_feature_groups
from etl.feature_engineering import load_holidays_df
import shap
import json


def _fmt_idr(x: Optional[float]) -> str:
    if x is None:
        return "‚Äî"
    try:
        return f"{int(round(float(x))):,} IDR".replace(",", " ")
    except Exception:
        return str(x)


def _fmt_pct(x: Optional[float], digits: int = 1) -> str:
    if x is None:
        return "‚Äî"
    return f"{x:.{digits}f}%"


def _fmt_rate(x: Optional[float], digits: int = 2) -> str:
    if x is None:
        return "‚Äî"
    return f"{x:.{digits}f}"


def _hms_from_minutes(total_minutes: float) -> str:
    m = max(total_minutes, 0)
    hours = int(m // 60)
    minutes = int(m % 60)
    seconds = int(round((m - int(m)) * 60))
    return f"{hours}:{minutes:02d}:{seconds:02d}"


def _sec_to_minutes(avg_seconds: float) -> float:
    return float(avg_seconds) / 60.0


def _section1_exec(basic: Dict) -> str:
    es = basic["executive_summary"]
    grab = es["by_platform"]["grab"]
    gojek = es["by_platform"]["gojek"]

    total_rev = _fmt_idr(es["revenue_total"])
    grab_rev = _fmt_idr(grab.get("sales"))
    gojek_rev = _fmt_idr(gojek.get("sales"))

    total_orders = es.get("orders_total") or 0
    # Optional enriched fields
    fake = es.get("fake_orders", {})
    canc = es.get("cancellations", {})
    lost = es.get("lost_orders", {})
    succ = es.get("successful_orders", {})
    aov = es.get("aov", {})

    lines = []
    lines.append("üìä 1. –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ï –†–ï–ó–Æ–ú–ï")
    lines.append("‚Äî" * 72)
    lines.append(f"üí∞ –û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞: {total_rev} (GRAB: {grab_rev} + GOJEK: {gojek_rev})")
    lines.append(f"üì¶ –û–±—â–∏–µ –∑–∞–∫–∞–∑—ã: {total_orders}")
    lines.append(f"   ‚îú‚îÄ‚îÄ üì± GRAB: {int(grab.get('orders') or 0)} (—É—Å–ø–µ—à–Ω–æ: {succ.get('grab','‚Äî')}, –æ—Ç–º–µ–Ω—ã: {canc.get('grab','‚Äî')}, –ø–æ—Ç–µ—Ä–∏: {lost.get('grab','‚Äî')}, fake: {fake.get('grab','‚Äî')})")
    lines.append(f"   ‚îî‚îÄ‚îÄ üõµ GOJEK: {int(gojek.get('orders') or 0)} (—É—Å–ø–µ—à–Ω–æ: {succ.get('gojek','‚Äî')}, –æ—Ç–º–µ–Ω—ã: {canc.get('gojek','‚Äî')}, –ø–æ—Ç–µ—Ä–∏: {lost.get('gojek','‚Äî')}, fake: {fake.get('gojek','‚Äî')})")
    # AOVs
    if aov:
        lines.append(f"üíµ –°—Ä–µ–¥–Ω–∏–π —á–µ–∫ (—É—Å–ø–µ—à–Ω—ã–µ): –æ–±—â–∏–π { _fmt_idr(aov.get('total')) }; GRAB { _fmt_idr(aov.get('grab')) }; GOJEK { _fmt_idr(aov.get('gojek')) }")
    # Daily revenue
    drw = es.get('daily_revenue_workdays_avg')
    if drw is not None:
        lines.append(f"üìä –î–Ω–µ–≤–Ω–∞—è –≤—ã—Ä—É—á–∫–∞: {_fmt_idr(drw)} (—Å—Ä–µ–¥–Ω—è—è –ø–æ —Ä–∞–±–æ—á–∏–º –¥–Ω—è–º)")
    # Rating
    rat = es.get('rating_avg_total')
    if rat:
        lines.append(f"‚≠ê –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {_fmt_rate(float(rat), 2)}/5.0")
    # Clients
    cli = es.get('clients', {})
    if cli:
        tot = cli.get('total_unique')
        lines.append(f"üë• –û–±—Å–ª—É–∂–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {tot if tot is not None else '‚Äî'}")
        g = cli.get('grab', {})
        j = cli.get('gojek', {})
        lines.append(f"   ‚îú‚îÄ‚îÄ üì± GRAB: {g.get('total','‚Äî')} (–Ω–æ–≤—ã–µ: {g.get('new','‚Äî')}, –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ: {g.get('repeated','‚Äî')}, —Ä–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {g.get('reactivated','‚Äî')})")
        lines.append(f"   ‚îî‚îÄ‚îÄ üõµ GOJEK: {j.get('new','‚Äî') + j.get('active','‚Äî') + j.get('returned','‚Äî') if all(isinstance(j.get(k), int) for k in ['new','active','returned']) else '‚Äî'} (–Ω–æ–≤—ã–µ: {j.get('new','‚Äî')}, –∞–∫—Ç–∏–≤–Ω—ã–µ: {j.get('active','‚Äî')}, –≤–æ–∑–≤—Ä–∞—Ç–∏–≤—à–∏–µ—Å—è: {j.get('returned','‚Äî')})")
        if tot is not None:
            lines.append(f"   üí° –û–±—â–∏–π –æ—Ö–≤–∞—Ç: {tot} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤")
    # Marketing budget
    mb = es.get('marketing_budget', {})
    if mb:
        total_b = mb.get('total') or 0.0
        lines.append(f"üí∏ –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–π –±—é–¥–∂–µ—Ç: {_fmt_idr(total_b)} ({_fmt_pct(mb.get('share_of_revenue_pct'))} –æ—Ç –≤—ã—Ä—É—á–∫–∏)")
        lines.append("üìä –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∑–∞—Ç—Ä–∞—Ç:")
        lines.append("   ‚îå‚îÄ üì± GRAB:")
        lines.append(f"   ‚îÇ  üí∞ –ë—é–¥–∂–µ—Ç: {_fmt_idr(mb.get('grab'))}")
        # Additional ratios require per-platform revenue; already printed above implicitly; keep budget split concise
        lines.append("   ‚îî‚îÄ üõµ GOJEK:")
        lines.append(f"      üí∞ –ë—é–¥–∂–µ—Ç: {_fmt_idr(mb.get('gojek'))}")
    # ROAS summary
    ro = es.get('roas', {})
    if ro:
        lines.append("")
        lines.append("üéØ ROAS –ê–ù–ê–õ–ò–ó:")
        lines.append(f"‚îú‚îÄ‚îÄ üì± GRAB: {_fmt_rate(ro.get('grab'))}x")
        lines.append(f"‚îú‚îÄ‚îÄ üõµ GOJEK: {_fmt_rate(ro.get('gojek'))}x")
        lines.append(f"‚îî‚îÄ‚îÄ üéØ –û–ë–©–ò–ô: {_fmt_rate(ro.get('total'))}x")
    return "\n".join(lines)


def _dataset_version_banner() -> str:
    try:
        metrics_path = "/workspace/ml/artifacts/metrics.json"
        if not os.path.exists(metrics_path):
            return ""
        with open(metrics_path, "r", encoding="utf-8") as f:
            m = json.load(f)
        h = str(m.get("dataset_hash",""))
        rows = m.get("dataset_rows")
        ts = m.get("run_at_utc")
        champ = m.get("champion")
        short = h[:10] if h else ""
        return f"–í–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {short} ¬∑ —Å—Ç—Ä–æ–∫: {rows} ¬∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–ª—Å—è: {ts} ¬∑ –º–æ–¥–µ–ª—å: {champ}"
    except Exception:
        return ""


def _section2_trends(basic: Dict) -> str:
    st = basic["sales_trends"]
    monthly = st.get("monthly", {})
    lines = []
    lines.append("üìà 2. –ê–ù–ê–õ–ò–ó –ü–†–û–î–ê–ñ –ò –¢–†–ï–ù–î–û–í")
    lines.append("‚Äî" * 72)
    lines.append("üìä –î–∏–Ω–∞–º–∏–∫–∞ –ø–æ –º–µ—Å—è—Ü–∞–º:")
    for ym in sorted(monthly.keys()):
        m = monthly[ym]
        lines.append(
            f"  {ym}: {_fmt_idr(m['total_sales'])} ({m['days']} –¥–Ω–µ–π, {_fmt_idr(m['avg_per_day'])}/–¥–µ–Ω—å)"
        )
    w = st.get("weekend_vs_weekday", {})
    lines.append("")
    lines.append("üóìÔ∏è –í—ã—Ö–æ–¥–Ω—ã–µ vs –ë—É–¥–Ω–∏:")
    lines.append(f"  üìÖ –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏ –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ: {_fmt_idr(w.get('weekend_avg'))}")
    lines.append(f"  üìÖ –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏ –≤ –±—É–¥–Ω–∏: {_fmt_idr(w.get('weekday_avg'))}")
    lines.append(f"  üìä –≠—Ñ—Ñ–µ–∫—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö: {_fmt_pct(w.get('effect_pct'))}")

    best = st.get("best_day")
    worst = st.get("worst_day")
    if best:
        gp = best.get("by_platform", {})
        lines.append("üìä –ê–ù–ê–õ–ò–ó –†–ê–ë–û–ß–ò–• –î–ù–ï–ô:")
        lines.append(
            f"üèÜ –õ—É—á—à–∏–π –¥–µ–Ω—å: {best['date']} - {_fmt_idr(best['total_sales'])}"
        )
        lines.append(
            f"   üí∞ GRAB: {_fmt_idr(gp.get('grab',{}).get('sales'))} ({int(gp.get('grab',{}).get('orders') or 0)} –∑–∞–∫–∞–∑–æ–≤) | "
            f"GOJEK: {_fmt_idr(gp.get('gojek',{}).get('sales'))} ({int(gp.get('gojek',{}).get('orders') or 0)} –∑–∞–∫–∞–∑–æ–≤)"
        )
    if worst:
        gp = worst.get("by_platform", {})
        lines.append(
            f"üìâ –•—É–¥—à–∏–π –¥–µ–Ω—å: {worst['date']} - {_fmt_idr(worst['total_sales'])}"
        )
        lines.append(
            f"   üí∞ GRAB: {_fmt_idr(gp.get('grab',{}).get('sales'))} | GOJEK: {_fmt_idr(gp.get('gojek',{}).get('sales'))}"
        )
    return "\n".join(lines)


def _section4_marketing(mkt: Dict) -> str:
    f = mkt.get("funnel_grab", {})
    rm = mkt.get("roas_by_month", {})
    lines = []
    lines.append("üìà 4. –ú–ê–†–ö–ï–¢–ò–ù–ì–û–í–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –ò –í–û–†–û–ù–ö–ê")
    lines.append("‚Äî" * 72)
    lines.append("üìä –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –≤–æ—Ä–æ–Ω–∫–∞ (—Ç–æ–ª—å–∫–æ GRAB):")
    lines.append(f"  üëÅÔ∏è –ü–æ–∫–∞–∑—ã —Ä–µ–∫–ª–∞–º—ã: {int(f.get('impressions') or 0)}")
    lines.append(
        f"  üîó –ü–æ—Å–µ—â–µ–Ω–∏—è –º–µ–Ω—é: {int(f.get('menu_visits') or 0)} (CTR: {_fmt_pct((f.get('ctr') or 0)*100)})"
    )
    lines.append(
        f"  üõí –î–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∫–æ—Ä–∑–∏–Ω—É: {int(f.get('add_to_cart') or 0)} (–∫–æ–Ω–≤–µ—Ä—Å–∏—è: {_fmt_pct((f.get('conv_click_to_order') or 0)*100)} –æ—Ç –∫–ª–∏–∫–æ–≤)"
    )
    lines.append(
        f"  üì¶ –ó–∞–∫–∞–∑—ã –æ—Ç —Ä–µ–∫–ª–∞–º—ã: {int(f.get('ads_orders') or 0)} (–∫–æ–Ω–≤–µ—Ä—Å–∏—è: {_fmt_pct((f.get('conv_cart_to_order') or 0)*100)} –æ—Ç –∫–æ—Ä–∑–∏–Ω—ã)"
    )
    lines.append("")
    lines.append("  üìä –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–í–ï–†–°–ò–ò:")
    lines.append(f"  ‚Ä¢ üéØ –ü–æ–∫–∞–∑ ‚Üí –ó–∞–∫–∞–∑: {_fmt_pct((f.get('show_to_order') or 0)*100)}")
    lines.append(f"  ‚Ä¢ üîó –ö–ª–∏–∫ ‚Üí –ó–∞–∫–∞–∑: {_fmt_pct((f.get('conv_click_to_order') or 0)*100)}")
    lines.append(f"  ‚Ä¢ üõí –ö–æ—Ä–∑–∏–Ω–∞ ‚Üí –ó–∞–∫–∞–∑: {_fmt_pct((f.get('conv_cart_to_order') or 0)*100)}")
    lines.append("")
    bouncers = int(max((f.get('menu_visits') or 0) - (f.get('add_to_cart') or 0), 0))
    aband = int(max((f.get('add_to_cart') or 0) - (f.get('ads_orders') or 0), 0))
    lines.append("  üìâ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –í–û–†–û–ù–ö–ò:")
    lines.append(
        f"  ‚Ä¢ üíî –î–æ–ª—è —É—à–µ–¥—à–∏—Ö –±–µ–∑ –ø–æ–∫—É–ø–∫–∏: {_fmt_pct((f.get('bounce_rate') or 0)*100)} ({bouncers} —É—à–ª–∏ –±–µ–∑ –ø–æ–∫—É–ø–∫–∏)"
    )
    lines.append(
        f"  ‚Ä¢ üõí –î–æ–ª—è –Ω–µ–æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã—Ö –∫–æ—Ä–∑–∏–Ω: {_fmt_pct((f.get('abandoned_carts_rate') or 0)*100)} ({aband} –¥–æ–±–∞–≤–∏–ª–∏, –Ω–æ –Ω–µ –∫—É–ø–∏–ª–∏)"
    )
    lines.append("")
    lines.append("  üí∞ –ü–û–¢–ï–ù–¶–ò–ê–õ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –í–û–†–û–ù–ö–ò:")
    up = f.get("uplift_estimations", {})
    lines.append(
        f"  ‚Ä¢ üìà –°–Ω–∏–∂–µ–Ω–∏–µ –¥–æ–ª–∏ —É—à–µ–¥—à–∏—Ö –Ω–∞ 10%: {_fmt_idr(up.get('reduce_bounce_10_pct_revenue'))}"
    )
    lines.append(
        f"  ‚Ä¢ üõí –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã—Ö –∫–æ—Ä–∑–∏–Ω: {_fmt_idr(up.get('eliminate_abandoned_revenue'))}"
    )
    lines.append(
        f"  ‚Ä¢ üéØ –û–±—â–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª: {_fmt_idr(up.get('total_uplift'))}"
    )
    lines.append("")
    lines.append("üí∏ –°—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è (GRAB):")
    lines.append(
        f"  üí∞ CPC: {_fmt_idr(f.get('cpc'))} (—Ä–∞—Å—á—ë—Ç: –±—é–¥–∂–µ—Ç √∑ –ø–æ—Å–µ—â–µ–Ω–∏—è –º–µ–Ω—é)" 
    )
    lines.append(
        f"  üí∞ CPA: {_fmt_idr(f.get('cpa'))} (—Ä–∞—Å—á—ë—Ç: –±—é–¥–∂–µ—Ç √∑ –∑–∞–∫–∞–∑—ã –æ—Ç —Ä–µ–∫–ª–∞–º—ã)"
    )
    return "\n".join(lines)


def _section5_finance(fin: Dict) -> str:
    lines = []
    lines.append("5. üí≥ –§–ò–ù–ê–ù–°–û–í–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò")
    lines.append("‚Äî" * 72)
    payouts = fin.get("payouts", {})
    total_payouts = payouts.get("total") or 0.0
    grab_p = payouts.get("grab") or 0.0
    gojek_p = payouts.get("gojek") or 0.0
    grab_pct = (grab_p / total_payouts * 100.0) if total_payouts else None
    gojek_pct = (gojek_p / total_payouts * 100.0) if total_payouts else None
    lines.append("üí∞ –í—ã–ø–ª–∞—Ç—ã:")
    lines.append(f"   ‚îú‚îÄ‚îÄ üì± GRAB: {_fmt_idr(grab_p)} ({_fmt_pct(grab_pct)})")
    lines.append(f"   ‚îú‚îÄ‚îÄ üõµ GOJEK: {_fmt_idr(gojek_p)} ({_fmt_pct(gojek_pct)})")
    lines.append(f"   ‚îî‚îÄ‚îÄ üíé –û–±—â–∏–µ –≤—ã–ø–ª–∞—Ç—ã: {_fmt_idr(total_payouts)}")

    ad_sales = fin.get("ad_sales")
    ad_share = (fin.get("ad_sales_share") or 0.0) * 100.0
    lines.append("üìä –†–µ–∫–ª–∞–º–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:")
    lines.append(f"   ‚îú‚îÄ‚îÄ üí∞ –û–±—â–∏–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏: {_fmt_idr(ad_sales)}")
    lines.append(f"   ‚îú‚îÄ‚îÄ üìà –î–æ–ª—è –æ—Ç –æ–±—â–∏—Ö –ø—Ä–æ–¥–∞–∂: {_fmt_pct(ad_share)}")
    lines.append(
        f"   ‚îú‚îÄ‚îÄ üéØ GRAB ROAS: {_fmt_rate(fin.get('roas',{}).get('grab'))}x"
    )
    lines.append(
        f"   ‚îî‚îÄ‚îÄ üéØ GOJEK ROAS: {_fmt_rate(fin.get('roas',{}).get('gojek'))}x"
    )

    tr = fin.get("take_rate", {})
    net_roas = fin.get("net_roas", {})
    lines.append("")
    lines.append("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:")
    lines.append(
        f"   ‚Ä¢ Take rate (–¥–æ–ª—è –∫–æ–º–∏—Å—Å–∏–π –∏ —É–¥–µ—Ä–∂–∞–Ω–∏–π): GRAB { _fmt_pct((tr.get('grab') or 0.0)*100) }, GOJEK { _fmt_pct((tr.get('gojek') or 0.0)*100) }"
    )
    lines.append(
        f"   ‚Ä¢ –ß–∏—Å—Ç—ã–π ROAS: GRAB {_fmt_rate(net_roas.get('grab'))}x; GOJEK {_fmt_rate(net_roas.get('gojek'))}x"
    )
    contrib = fin.get("contribution_per_ad_order_grab")
    if contrib is not None:
        lines.append(
            f"   ‚Ä¢ –Æ–Ω–∏—Ç‚Äë—ç–∫–æ–Ω–æ–º–∏–∫–∞ —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ –∑–∞–∫–∞–∑–∞ (GRAB): {_fmt_idr(contrib)}"
        )
    return "\n".join(lines)


def _parse_time_to_minutes(val: str) -> Optional[float]:
    if val is None:
        return None
    s = str(val)
    parts = s.split(":")
    try:
        if len(parts) == 3:
            h, m, sec = parts
            return int(h) * 60 + int(m) + int(sec) / 60.0
        if len(parts) == 2:
            h, m = parts
            return int(h) * 60 + int(m)
        # fallback numeric
        return float(s)
    except Exception:
        return None


def _section6_operations(period: str, restaurant_id: int) -> str:
    eng = get_engine()
    start_str, end_str = period.split("_")
    start = pd.to_datetime(start_str)
    end = pd.to_datetime(end_str)

    # GRAB driver_waiting_time JSON average (seconds -> minutes if needed)
    qg = (
        "SELECT driver_waiting_time FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ? "
        "AND driver_waiting_time IS NOT NULL"
    )
    g = pd.read_sql_query(qg, eng, params=(restaurant_id, start_str, end_str))
    grab_wait_vals = []
    for v in g['driver_waiting_time'].dropna().tolist():
        try:
            if isinstance(v, str) and v.strip().startswith('{'):
                d = pd.read_json(pd.io.common.StringIO(v), typ='series') if False else None
                # Fallback manual parse
                import json as _json
                d = _json.loads(v)
                for k in ('avg','average','minutes','mean'):
                    if k in d:
                        val = float(d[k])
                        # heuristic: if seconds, convert to minutes
                        grab_wait_vals.append(val/60.0 if val > 60 else val)
                        break
            elif isinstance(v, (int, float)):
                val = float(v)
                grab_wait_vals.append(val/60.0 if val > 60 else val)
        except Exception:
            pass
    grab_wait_avg = sum(grab_wait_vals)/len(grab_wait_vals) if grab_wait_vals else None

    # GOJEK times
    qj = (
        "SELECT accepting_time, preparation_time, delivery_time, driver_waiting, close_time, stat_date "
        "FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?"
    )
    j = pd.read_sql_query(qj, eng, params=(restaurant_id, start_str, end_str))
    acc = pd.Series([_parse_time_to_minutes(x) for x in j['accepting_time'] if pd.notna(x)])
    prep = pd.Series([_parse_time_to_minutes(x) for x in j['preparation_time'] if pd.notna(x)])
    delv = pd.Series([_parse_time_to_minutes(x) for x in j['delivery_time'] if pd.notna(x)])
    drvw = pd.to_numeric(j['driver_waiting'], errors='coerce').dropna()

    # cancellations
    Cg = pd.read_sql_query(
        "SELECT SUM(cancelled_orders) c FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]['c'] or 0
    Cj_row = pd.read_sql_query(
        "SELECT SUM(cancelled_orders) c, SUM(lost_orders) l FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]
    Cj = Cj_row['c'] or 0; Lj = Cj_row['l'] or 0
    orders_total = (
        (pd.read_sql_query("SELECT SUM(orders) o FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?", eng, params=(restaurant_id, start_str, end_str)).iloc[0]['o'] or 0)
        + (pd.read_sql_query("SELECT SUM(orders) o FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?", eng, params=(restaurant_id, start_str, end_str)).iloc[0]['o'] or 0)
    )
    cancel_rate = ((Cg + Cj) / orders_total * 100.0) if orders_total else None

    # outages events > 1 hour
    events = []
    # GRAB offline_rate in minutes
    og = pd.read_sql_query(
        "SELECT stat_date, offline_rate FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ? AND offline_rate IS NOT NULL",
        eng, params=(restaurant_id, start_str, end_str)
    )
    for _, row in og.iterrows():
        mins = float(row['offline_rate'] or 0)
        if mins >= 60.0:
            events.append((pd.to_datetime(row['stat_date']).date(), 'GRAB', mins/60.0))
    # GOJEK close_time HH:MM:SS
    oj = pd.read_sql_query(
        "SELECT stat_date, close_time FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    )
    for _, row in oj.iterrows():
        ct = str(row['close_time']) if pd.notna(row['close_time']) else ''
        parts = ct.split(':')
        seconds = None
        try:
            if len(parts) == 3:
                h, m, s = parts
                seconds = int(h)*3600 + int(m)*60 + int(s)
        except Exception:
            seconds = None
        if seconds and seconds >= 3600:
            events.append((pd.to_datetime(row['stat_date']).date(), 'GOJEK', seconds/3600.0))

    # Potential losses
    # average hourly revenue by platform
    # platform sales
    sg = pd.read_sql_query(
        "SELECT SUM(sales) s FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]['s'] or 0.0
    sj = pd.read_sql_query(
        "SELECT SUM(sales) s FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]['s'] or 0.0
    num_days = (end - start).days + 1
    hr_g = (sg / (num_days*24.0)) if num_days>0 else 0.0
    hr_j = (sj / (num_days*24.0)) if num_days>0 else 0.0

    total_loss_g = sum((hrs*hr_g) for (d,plat,hrs) in events if plat=='GRAB')
    total_loss_j = sum((hrs*hr_j) for (d,plat,hrs) in events if plat=='GOJEK')

    lines = []
    lines.append("6. ‚è∞ –û–ü–ï–†–ê–¶–ò–û–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò")
    lines.append("‚Äî" * 72)
    lines.append("üü¢ GRAB:")
    lines.append(f"‚îî‚îÄ‚îÄ ‚è∞ –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤–æ–¥–∏—Ç–µ–ª–µ–π: {_fmt_rate(grab_wait_avg,1)} –º–∏–Ω")
    lines.append("")
    lines.append("üü† GOJEK:")
    lines.append(f"‚îú‚îÄ‚îÄ ‚è±Ô∏è –í—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è: {_fmt_rate(prep.mean() if not prep.empty else None,1)} –º–∏–Ω")
    lines.append(f"‚îú‚îÄ‚îÄ üöó –í—Ä–µ–º—è –¥–æ—Å—Ç–∞–≤–∫–∏: {_fmt_rate(delv.mean() if not delv.empty else None,1)} –º–∏–Ω  ")
    lines.append(f"‚îî‚îÄ‚îÄ ‚è∞ –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤–æ–¥–∏—Ç–µ–ª–µ–π: {_fmt_rate(drvw.mean() if not drvw.empty else None,1)} –º–∏–Ω")
    lines.append("")
    lines.append("‚ö†Ô∏è –û–ü–ï–†–ê–¶–ò–û–ù–ù–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨")
    lines.append("‚Äî" * 72)
    lines.append("üö´ –û—Ç–º–µ–Ω–µ–Ω–Ω—ã–µ –∑–∞–∫–∞–∑—ã:")
    lines.append(f"   ‚îú‚îÄ‚îÄ üì± GRAB: {int(Cg)} –∑–∞–∫–∞–∑–∞")
    lines.append(f"   ‚îî‚îÄ‚îÄ üõµ GOJEK: {int(Cj)} –∑–∞–∫–∞–∑–∞")
    lines.append(f"   üí° –í—Å–µ–≥–æ –æ—Ç–º–µ–Ω–µ–Ω–Ω—ã—Ö: {int(Cg+Cj)} –∑–∞–∫–∞–∑–æ–≤ ({_fmt_pct(cancel_rate)})")
    lines.append("")
    if events:
        total_loss = total_loss_g + total_loss_j
        total_sales = sg + sj
        loss_pct = (total_loss/total_sales*100.0) if total_sales else None
        lines.append("üîß –û–ü–ï–†–ê–¶–ò–û–ù–ù–´–ï –°–ë–û–ò –ü–õ–ê–¢–§–û–†–ú:")
        # aggregate durations per platform
        dur_g = sum(hrs for (_,plat,hrs) in events if plat=='GRAB')
        dur_j = sum(hrs for (_,plat,hrs) in events if plat=='GOJEK')
        from datetime import timedelta
        def hms_from_hours(h):
            h_int = int(h)
            m = int((h - h_int)*60)
            s = int(round(((h - h_int)*60 - m)*60))
            return f"{h_int}:{m:02d}:{s:02d}"
        lines.append(f"‚îú‚îÄ‚îÄ üì± GRAB: {len([1 for _,p,_ in events if p=='GRAB'])} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –¥–Ω—è ({hms_from_hours(dur_g)} –æ–±—â–µ–µ –≤—Ä–µ–º—è)")
        lines.append(f"‚îú‚îÄ‚îÄ üõµ GOJEK: {len([1 for _,p,_ in events if p=='GOJEK'])} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –¥–Ω—è ({hms_from_hours(dur_j)} –æ–±—â–µ–µ –≤—Ä–µ–º—è)")
        lines.append(f"‚îî‚îÄ‚îÄ üí∏ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏: {_fmt_idr(total_loss)} ({_fmt_pct(loss_pct)})")
        if events:
            lines.append("")
            lines.append("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –°–ë–û–ò (>1 —á–∞—Å–∞):")
            # sort by date
            for d, plat, hrs in sorted(events, key=lambda x: x[0]):
                loss = hrs*(hr_g if plat=='GRAB' else hr_j)
                lines.append(f"   ‚Ä¢ {d}: {plat} offline {hms_from_hours(hrs)} (–ø–æ—Ç–µ—Ä–∏: ~{_fmt_idr(loss)})")
    return "\n".join(lines)


def _section3_clients(period: str, restaurant_id: int) -> str:
    eng = get_engine()
    start_str, end_str = period.split("_")
    qg = (
        "SELECT SUM(new_customers) new, SUM(repeated_customers) rep, SUM(reactivated_customers) rea, SUM(total_customers) tot, "
        "SUM(earned_new_customers) enew, SUM(earned_repeated_customers) erep, SUM(earned_reactivated_customers) erea "
        "FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?"
    )
    qj = (
        "SELECT SUM(new_client) new, SUM(active_client) act, SUM(returned_client) ret "
        "FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?"
    )
    g = pd.read_sql_query(qg, eng, params=(restaurant_id, start_str, end_str)).iloc[0].fillna(0)
    j = pd.read_sql_query(qj, eng, params=(restaurant_id, start_str, end_str)).iloc[0].fillna(0)
    grab_new, grab_rep, grab_rea, grab_tot = int(g['new']), int(g['rep']), int(g['rea']), int(g['tot'])
    gojek_new, gojek_act, gojek_ret = int(j['new']), int(j['act']), int(j['ret'])
    total_unique = grab_tot + gojek_new + gojek_act + gojek_ret  # –≤–µ—Ä—Ö–Ω—è—è –æ—Ü–µ–Ω–∫–∞

    lines = []
    lines.append("üë• 3. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–õ–ò–ï–ù–¢–°–ö–û–ô –ë–ê–ó–´")
    lines.append("‚Äî" * 72)
    lines.append("üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π –±–∞–∑—ã (GRAB + GOJEK):")
    lines.append(f"  üÜï –ù–æ–≤—ã–µ –∫–ª–∏–µ–Ω—Ç—ã: {grab_new + gojek_new}")
    lines.append(f"    üì± GRAB: {grab_new} | üõµ GOJEK: {gojek_new}")
    lines.append(f"  üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã: {grab_rep + gojek_act}")
    lines.append(f"    üì± GRAB: {grab_rep} | üõµ GOJEK: {gojek_act}")
    lines.append(f"  üì≤ –†–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {grab_rea + gojek_ret}")
    lines.append(f"    üì± GRAB: {grab_rea} | üõµ GOJEK: {gojek_ret}")
    lines.append("")
    lines.append("üí∞ –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ GRAB, —Ç–æ–ª—å–∫–æ —Å —Ä–µ–∫–ª–∞–º—ã):")
    lines.append(f"  üÜï –ù–æ–≤—ã–µ: {_fmt_idr(g['enew'])}")
    lines.append(f"  üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ: {_fmt_idr(g['erep'])}")
    lines.append(f"  üì≤ –†–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {_fmt_idr(g['erea'])}")
    return "\n".join(lines)


def _section7_quality(quality: Dict) -> str:
    r = quality.get("ratings", {})
    lines = []
    lines.append("7. ‚≠ê –ö–ê–ß–ï–°–¢–í–û –û–ë–°–õ–£–ñ–ò–í–ê–ù–ò–Ø –ò –£–î–û–í–õ–ï–¢–í–û–†–ï–ù–ù–û–°–¢–¨ (GOJEK)")
    lines.append("‚Äî" * 72)
    total = r.get("total") or 0
    lines.append(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ (–≤—Å–µ–≥–æ: {total}):")
    lines.append(f"  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 5 –∑–≤–µ–∑–¥: {r.get('five',0)} ({_fmt_pct((r.get('five',0)/total*100) if total else None)})")
    lines.append(f"  ‚≠ê‚≠ê‚≠ê‚≠ê 4 –∑–≤–µ–∑–¥—ã: {r.get('four',0)} ({_fmt_pct((r.get('four',0)/total*100) if total else None)})")
    lines.append(f"  ‚≠ê‚≠ê‚≠ê 3 –∑–≤–µ–∑–¥—ã: {r.get('three',0)} ({_fmt_pct((r.get('three',0)/total*100) if total else None)})")
    lines.append(f"  ‚≠ê‚≠ê 2 –∑–≤–µ–∑–¥—ã: {r.get('two',0)} ({_fmt_pct((r.get('two',0)/total*100) if total else None)})")
    lines.append(f"  ‚≠ê 1 –∑–≤–µ–∑–¥–∞: {r.get('one',0)} ({_fmt_pct((r.get('one',0)/total*100) if total else None)})")
    lines.append("")
    lines.append(f"üìà –ò–Ω–¥–µ–∫—Å —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {_fmt_rate(r.get('satisfaction_index'))}/5.0")
    lines.append(f"üö® –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (1-2‚òÖ): {r.get('negative_1_2',{}).get('count',0)} ({_fmt_pct(r.get('negative_1_2',{}).get('percent'))})")
    lines.append("")
    lines.append("üìä –ß–∞—Å—Ç–æ—Ç–∞ –ø–ª–æ—Ö–∏—Ö –æ—Ü–µ–Ω–æ–∫ (–Ω–µ 5‚òÖ):")
    lines.append(f"  üìà –ü–ª–æ—Ö–∏—Ö –æ—Ü–µ–Ω–æ–∫ –≤—Å–µ–≥–æ: {r.get('not_five',{}).get('count',0)} –∏–∑ {total} ({_fmt_pct(r.get('not_five',{}).get('percent'))})")
    lines.append(f"  üì¶ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ GOJEK –Ω–∞ 1 –ø–ª–æ—Ö—É—é –æ—Ü–µ–Ω–∫—É: {_fmt_rate(quality.get('orders_per_not_five_rating'))}")
    return "\n".join(lines)


def _fmt_minutes_to_hhmmss(mins: Optional[float]) -> str:
    if mins is None or (isinstance(mins, float) and np.isnan(mins)):
        return "‚Äî"
    try:
        total_seconds = int(round(float(mins) * 60))
        h = total_seconds // 3600
        m = (total_seconds % 3600) // 60
        s = total_seconds % 60
        return f"{h}:{m:02d}:{s:02d}"
    except Exception:
        return "‚Äî"


def _categorize_feature(name: str) -> str:
    n = name.lower()
    if n.startswith("mkt_") or "ads_spend" in n or "impressions" in n or "roas" in n:
        return "Marketing"
    if n.startswith("ops_") or any(k in n for k in ["accepting_time", "preparation_time", "delivery_time", "outage_", "offline_"]):
        return "Operations"
    if n in ("temp", "rain", "wind", "humidity", "tourist_flow", "is_holiday", "day_of_week", "is_weekend") or any(k in n for k in ["temp_", "rain_", "wind_", "humidity_", "tourist_flow_"]):
        return "External"
    if "rating" in n:
        return "Quality"
    return "Other"


def _pretty_feature_name(name: str) -> str:
    n = name.lower()
    mapping = {
        # Marketing
        "mkt_roas_grab": "ROAS (GRAB)",
        "mkt_roas_gojek": "ROAS (GOJEK)",
        "mkt_ads_spend_grab": "–†–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç (GRAB)",
        "mkt_ads_spend_gojek": "–†–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç (GOJEK)",
        "ads_spend_total": "–†–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç (—Å—É–º–º–∞—Ä–Ω–æ)",
        "impressions_total": "–ü–æ–∫–∞–∑—ã —Ä–µ–∫–ª–∞–º—ã",
        # Operations (–æ–±—â–∏–µ)
        "preparation_time_mean": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è (–º–∏–Ω)",
        "accepting_time_mean": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (–º–∏–Ω)",
        "delivery_time_mean": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –¥–æ—Å—Ç–∞–≤–∫–∏ (–º–∏–Ω)",
        # Operations (GOJEK)
        "ops_preparation_time_gojek": "GOJEK: –≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è",
        "ops_accepting_time_gojek": "GOJEK: –≤—Ä–µ–º—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è",
        "ops_delivery_time_gojek": "GOJEK: –≤—Ä–µ–º—è –¥–æ—Å—Ç–∞–≤–∫–∏",
        # Outage/offline
        "outage_offline_rate_grab": "GRAB: –æ—Ñ—Ñ–ª–∞–π–Ω (–º–∏–Ω)",
        "offline_rate_grab": "GRAB: –æ—Ñ—Ñ–ª–∞–π–Ω (–º–∏–Ω)",
        # External
        "rain": "–î–æ–∂–¥—å (–º–º)",
        "temp": "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (¬∞C)",
        "wind": "–í–µ—Ç–µ—Ä",
        "humidity": "–í–ª–∞–∂–Ω–æ—Å—Ç—å (%)",
        "tourist_flow": "–¢—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫",
        "is_holiday": "–ü—Ä–∞–∑–¥–Ω–∏–∫",
        "day_of_week": "–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏",
        "is_weekend": "–í—ã—Ö–æ–¥–Ω–æ–π",
        # Quality
        "rating": "–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥",
    }
    if n in mapping:
        return mapping[n]
    # Heuristics: platform/time metrics
    if n.startswith("ops_preparation_time_"):
        plat = n.split("_")[-1].upper()
        return f"{plat}: –≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è"
    if n.startswith("ops_accepting_time_"):
        plat = n.split("_")[-1].upper()
        return f"{plat}: –≤—Ä–µ–º—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"
    if n.startswith("ops_delivery_time_"):
        plat = n.split("_")[-1].upper()
        return f"{plat}: –≤—Ä–µ–º—è –¥–æ—Å—Ç–∞–≤–∫–∏"
    if n.startswith("mkt_roas_"):
        plat = n.split("_")[-1].upper()
        return f"ROAS ({plat})"
    if n.startswith("mkt_ads_spend_"):
        plat = n.split("_")[-1].upper()
        return f"–†–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç ({plat})"
    # Fallback: make readable
    pretty = name.replace("_", " ")
    pretty = pretty.replace("grab", "GRAB").replace("gojek", "GOJEK").replace("roas", "ROAS")
    return pretty


# –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–∞ 8
MIN_NEG_SHARE = 0.02  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–∫–ª–∞–¥–∞ (2%)
MIN_NEG_IDR = 100000  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∫–ª–∞–¥ –≤ IDR (100K IDR)
REPORT_STRICT_MODE = True  # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö

def _normalize_feature_name(feature: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∏—á –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ"""
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥—É–±–ª–µ—Ä—ã –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è
    if any(x in feature.lower() for x in ['prep_time', 'preparation_time']):
        return "–í—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è"
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–≥–æ–¥–Ω—ã–µ –ª–∞–≥–∏
    if any(x in feature.lower() for x in ['humidity_lag', 'wind_lag', 'temp_lag', 'rain_lag']):
        weather_type = ""
        if 'humidity' in feature.lower():
            weather_type = "–≤–ª–∞–∂–Ω–æ—Å—Ç—å"
        elif 'wind' in feature.lower():
            weather_type = "–≤–µ—Ç–µ—Ä"
        elif 'temp' in feature.lower():
            weather_type = "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞"
        elif 'rain' in feature.lower():
            weather_type = "–¥–æ–∂–¥—å"
        return f"–ü–æ–≥–æ–¥–∞: {weather_type} (–ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–Ω–∏)"
    
    # –¢–µ–∫—É—â–∏–µ –ø–æ–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
    if feature.lower() in ['rain', 'humidity', 'wind', 'temp', 'temperature']:
        weather_map = {
            'rain': '–î–æ–∂–¥—å',
            'humidity': '–í–ª–∞–∂–Ω–æ—Å—Ç—å',
            'wind': '–í–µ—Ç–µ—Ä',
            'temp': '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞',
            'temperature': '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞'
        }
        return f"–ü–æ–≥–æ–¥–∞: {weather_map.get(feature.lower(), feature)}"
    
    # –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    marketing_map = {
        'ads_spend': '–†–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç',
        'ads_sales': '–†–µ–∫–ª–∞–º–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏',
        'roas': 'ROAS',
        'grab_ads_spend': 'GRAB: —Ä–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç',
        'gojek_ads_spend': 'GOJEK: —Ä–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç',
        'grab_roas': 'GRAB: ROAS',
        'gojek_roas': 'GOJEK: ROAS'
    }
    
    for key, value in marketing_map.items():
        if key in feature.lower():
            return value
    
    # –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    ops_map = {
        'confirm_time': '–í—Ä–µ–º—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è',
        'delivery_time': '–í—Ä–µ–º—è –¥–æ—Å—Ç–∞–≤–∫–∏',
        'rating': '–†–µ–π—Ç–∏–Ω–≥',
        'cancelled_orders': '–û—Ç–º–µ–Ω—ã',
        'lost_orders': '–ü–æ—Ç–µ—Ä–∏'
    }
    
    for key, value in ops_map.items():
        if key in feature.lower():
            return value
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
    return _pretty_feature_name(feature)


def _section8_critical_days_ml(period: str, restaurant_id: int) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª 8 —Å –ø–æ—Ä–æ–≥–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏ —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    try:
        start_str, end_str = period.split("_")
        df = pd.read_csv("/workspace/data/merged_dataset.csv", parse_dates=["date"])
        sub = df[(df["restaurant_id"] == restaurant_id) & (df["date"] >= start_str) & (df["date"] <= end_str)].copy()
        
        if sub.empty:
            return "8. üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –î–ù–ò\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥."
        
        # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if REPORT_STRICT_MODE and len(sub) < 7:
            return "8. üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –î–ù–ò\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n–î–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º—É–º 7 –¥–Ω–µ–π)."

        # –ù–∞—Ö–æ–¥–∏–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–Ω–∏ (–ø–∞–¥–µ–Ω–∏–µ ‚â•30% –æ—Ç –º–µ–¥–∏–∞–Ω—ã)
        daily = sub.groupby("date", as_index=False)["total_sales"].sum().sort_values("date")
        median_sales = float(daily["total_sales"].median()) if len(daily) else 0.0
        threshold = 0.70 * median_sales  # 30% –ø–∞–¥–µ–Ω–∏–µ –æ—Ç –º–µ–¥–∏–∞–Ω—ã
        critical_dates = daily.loc[daily["total_sales"] <= threshold, "date"].dt.normalize().tolist()
        critical_dates = sorted(critical_dates, key=lambda d: daily.loc[daily["date"] == d, "total_sales"].iloc[0])

        lines: list[str] = []
        lines.append("8. üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –î–ù–ò")
        lines.append("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        lines.append(f"üìä –ù–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –¥–Ω–µ–π (–ø–∞–¥–µ–Ω–∏–µ ‚â•30%): {len(critical_dates)} –∏–∑ {len(daily)} ({len(critical_dates)/len(daily)*100:.1f}%)")
        lines.append(f"üìà –ú–µ–¥–∏–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏: {_fmt_idr(median_sales)}")
        lines.append(f"üìâ –ü–æ—Ä–æ–≥ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏: {_fmt_idr(threshold)}")
        
        if not critical_dates:
            lines.append("")
            lines.append("‚úÖ –í –ø–µ—Ä–∏–æ–¥–µ –Ω–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –¥–Ω–µ–π (–ø–∞–¥–µ–Ω–∏–µ ‚â•30%)")
            return "\\n".join(lines)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–∏–µ –ø–æ—Ç–µ—Ä–∏
        total_losses = 0.0
        for critical_date in critical_dates:
            day_sales = daily.loc[daily["date"] == critical_date, "total_sales"].iloc[0]
            loss = max(median_sales - day_sales, 0)
            total_losses += loss
        
        lines.append(f"üí∏ –û–±—â–∏–µ –ø–æ—Ç–µ—Ä–∏ –æ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –¥–Ω–µ–π: {_fmt_idr(total_losses)}")
        lines.append("")
        
        def _analyze_critical_day_improved(critical_date: pd.Timestamp) -> list[str]:
            """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–Ω—è —Å –ø–æ—Ä–æ–≥–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏"""
            day_lines = []
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–Ω—è
            day_data = sub[sub["date"] == critical_date].iloc[0] if not sub[sub["date"] == critical_date].empty else None
            if day_data is None:
                return [f"üî¥ {critical_date.strftime('%Y-%m-%d')}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"]
            
            day_sales = float(day_data["total_sales"])
            loss_amount = max(median_sales - day_sales, 0)
            loss_pct = ((day_sales - median_sales) / median_sales * 100) if median_sales > 0 else 0
            
            day_lines.append(f"üî¥ {critical_date.strftime('%Y-%m-%d')}")
            day_lines.append("")
            
            # –ö–ª—é—á–µ–≤—ã–µ —Ü–∏—Ñ—Ä—ã
            day_lines.append("### üìä **–ö–õ–Æ–ß–ï–í–´–ï –¶–ò–§–†–´**")
            day_lines.append(f"- **–ü—Ä–æ–¥–∞–∂–∏:** {_fmt_idr(day_sales)} (–º–µ–¥–∏–∞–Ω–∞: {_fmt_idr(median_sales)}) ‚Üí **{loss_pct:+.1f}%**")
            day_lines.append(f"- **–ü–æ—Ç–µ—Ä–∏:** {_fmt_idr(loss_amount)}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–∫–∞–∑—ã –∏ —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
            orders = day_data.get("orders_count", 0) or 0
            if orders > 0:
                avg_check = day_sales / orders
                day_lines.append(f"- **–ó–∞–∫–∞–∑—ã:** {orders} —à—Ç")
                day_lines.append(f"- **–°—Ä–µ–¥–Ω–∏–π —á–µ–∫:** {_fmt_idr(avg_check)}")
            
            day_lines.append("")
            
            # ML –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω —Å –ø–æ—Ä–æ–≥–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            try:
                model, features, background = load_artifacts()
                if model is None or not features:
                    if REPORT_STRICT_MODE:
                        day_lines.append("### ‚ö†Ô∏è **–ê–ù–ê–õ–ò–ó –ù–ï–î–û–°–¢–£–ü–ï–ù**")
                        day_lines.append("ML –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.")
                        return day_lines
                
                # –ü–æ–ª—É—á–∞–µ–º SHAP –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –¥–Ω—è
                day_features = day_data[features] if all(f in day_data.index for f in features) else None
                if day_features is None:
                    if REPORT_STRICT_MODE:
                        day_lines.append("### ‚ö†Ô∏è **–î–ê–ù–ù–´–• –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û**")
                        day_lines.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ features –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞.")
                        return day_lines
                
                X_day = day_features.values.reshape(1, -1)
                pre = model.named_steps["pre"]
                mdl = model.named_steps["model"]
                X_pre = pre.transform(X_day)
                
                if background is not None and not background.empty:
                    bg_pre = pre.transform(background[features])
                    explainer = shap.TreeExplainer(mdl, data=bg_pre, feature_perturbation="interventional")
                else:
                    explainer = shap.TreeExplainer(mdl, feature_perturbation="interventional")
                
                shap_values = explainer.shap_values(X_pre)[0]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –ø–æ –ø–æ—Ä–æ–≥–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
                negative_factors = []
                positive_factors = []
                
                for i, (feature, shap_val) in enumerate(zip(features, shap_values)):
                    if shap_val < 0:  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –≤–∫–ª–∞–¥
                        contribution_idr = abs(shap_val)
                        contribution_share = abs(shap_val) / loss_amount if loss_amount > 0 else 0
                        
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                        if contribution_share >= MIN_NEG_SHARE and contribution_idr >= MIN_NEG_IDR:
                            normalized_name = _normalize_feature_name(feature)
                            negative_factors.append((normalized_name, contribution_idr, contribution_share * 100))
                    
                    elif shap_val > 0:  # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π –≤–∫–ª–∞–¥ (—á—Ç–æ –ø–æ–º–æ–≥–ª–æ)
                        contribution_idr = shap_val
                        normalized_name = _normalize_feature_name(feature)
                        positive_factors.append((normalized_name, contribution_idr))
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∫–ª–∞–¥—É –∏ –±–µ—Ä–µ–º —Ç–æ–ø-5
                negative_factors.sort(key=lambda x: x[1], reverse=True)
                negative_factors = negative_factors[:5]
                
                positive_factors.sort(key=lambda x: x[1], reverse=True)
                positive_factors = positive_factors[:3]  # –¢–æ–ø-3 –ø–æ–º–æ–≥–∞—é—â–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–∞
                
                # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ—Ä–æ–≤
                if REPORT_STRICT_MODE and len(negative_factors) < 2:
                    day_lines.append("### ‚ö†Ô∏è **–î–ê–ù–ù–´–• –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û**")
                    day_lines.append("–ù–∞–π–¥–µ–Ω–æ –º–µ–Ω–µ–µ 2 –∑–Ω–∞—á–∏–º—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤. ML –∞–Ω–∞–ª–∏–∑ –Ω–µ—Ç–æ—á–µ–Ω.")
                    return day_lines
                
                # –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã
                day_lines.append("### üîç **–†–ï–ê–õ–¨–ù–´–ï –ü–†–ò–ß–ò–ù–´**")
                
                for i, (factor_name, contribution_idr, contribution_pct) in enumerate(negative_factors, 1):
                    priority = "üî¥" if contribution_pct >= 15.0 else ("üü°" if contribution_pct >= 7.0 else "üü¢")
                    day_lines.append(f"**{i}. {priority} {factor_name.upper()}**")
                    day_lines.append(f"- **–í–ª–∏—è–Ω–∏–µ:** {_fmt_idr(contribution_idr)} ({contribution_pct:.1f}% –æ—Ç –ø–æ—Ç–µ—Ä—å)")
                    day_lines.append("")
                
                # –í–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã (–ø—Ä–∞–∑–¥–Ω–∏–∫–∏ –∏ –ø–æ–≥–æ–¥–∞) —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞—é—Ç –ø–æ—Ä–æ–≥–∏
                day_lines.append("### üåç **–í–ù–ï–®–ù–ò–ï –§–ê–ö–¢–û–†–´**")
                
                # –ü—Ä–∞–∑–¥–Ω–∏–∫–∏ - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ holiday_flag==1 –∏ –≤–∫–ª–∞–¥ >= –ø–æ—Ä–æ–≥–∞
                is_holiday = int(day_data.get("is_holiday", 0)) == 1
                holiday_contribution = 0
                
                # –ò—â–µ–º –≤–∫–ª–∞–¥ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞ –≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–∞—Ö
                for factor_name, contribution_idr, contribution_pct in negative_factors:
                    if "–ø—Ä–∞–∑–¥–Ω–∏–∫" in factor_name.lower() or "holiday" in factor_name.lower():
                        holiday_contribution = contribution_idr
                        break
                
                if is_holiday and holiday_contribution >= MIN_NEG_IDR:
                    holiday_info = _check_holiday_by_date_simple(critical_date.strftime('%Y-%m-%d'))
                    day_lines.append(f"**üïå –ü—Ä–∞–∑–¥–Ω–∏–∫–∏:** {holiday_info}")
                elif is_holiday:
                    holiday_info = _check_holiday_by_date_simple(critical_date.strftime('%Y-%m-%d'))
                    day_lines.append(f"**üïå –ü—Ä–∞–∑–¥–Ω–∏–∫–∏:** {holiday_info} (–≤–ª–∏—è–Ω–∏–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ)")
                else:
                    day_lines.append("**üïå –ü—Ä–∞–∑–¥–Ω–∏–∫–∏:** –æ–±—ã—á–Ω—ã–π –¥–µ–Ω—å")
                
                # –î–æ–∂–¥—å - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ rain_mm >= 2.0 –∏ –≤–∫–ª–∞–¥ >= –ø–æ—Ä–æ–≥–∞
                rain_mm = float(day_data.get("rain", 0)) if pd.notna(day_data.get("rain")) else 0.0
                rain_contribution = 0
                
                # –ò—â–µ–º –≤–∫–ª–∞–¥ –¥–æ–∂–¥—è –≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–∞—Ö
                for factor_name, contribution_idr, contribution_pct in negative_factors:
                    if "–¥–æ–∂–¥—å" in factor_name.lower() or "rain" in factor_name.lower():
                        rain_contribution = contribution_idr
                        break
                
                if rain_mm >= 2.0 and rain_contribution >= MIN_NEG_IDR:
                    rain_desc = "—Å–∏–ª—å–Ω—ã–π –¥–æ–∂–¥—å" if rain_mm >= 10.0 else "–¥–æ–∂–¥—å"
                    day_lines.append(f"**üåßÔ∏è –ü–æ–≥–æ–¥–∞:** {rain_desc} {rain_mm:.1f}–º–º ‚Äî —Å–Ω–∏–∑–∏–ª –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—É—Ä—å–µ—Ä–æ–≤")
                elif rain_mm >= 2.0:
                    day_lines.append(f"**üåßÔ∏è –ü–æ–≥–æ–¥–∞:** –¥–æ–∂–¥—å {rain_mm:.1f}–º–º (–≤–ª–∏—è–Ω–∏–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ)")
                else:
                    temp = day_data.get("temp", 0) or 0
                    day_lines.append(f"**üåßÔ∏è –ü–æ–≥–æ–¥–∞:** –±–µ–∑ –¥–æ–∂–¥—è, –∫–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ {temp}¬∞C")
                
                day_lines.append("")
                
                # –ß—Ç–æ –ø–æ–º–æ–≥–ª–æ –∏–∑–±–µ–∂–∞—Ç—å –±–æ–ª—å—à–∏—Ö –ø–æ—Ç–µ—Ä—å (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã)
                if positive_factors:
                    day_lines.append("### ‚úÖ **–ß–¢–û –ü–û–ú–û–ì–õ–û –ò–ó–ë–ï–ñ–ê–¢–¨ –ë–û–õ–¨–®–ò–• –ü–û–¢–ï–†–¨**")
                    for factor_name, contribution_idr in positive_factors:
                        day_lines.append(f"**üí™ {factor_name}:**")
                        day_lines.append(f"- –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç: +{_fmt_idr(contribution_idr)}")
                    day_lines.append("")
                
                # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º —ç—Ñ—Ñ–µ–∫—Ç–æ–º
                day_lines.append("### üéØ **–ö–û–ù–ö–†–ï–¢–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò**")
                
                recommendations = []
                total_potential = 0
                
                for i, (factor_name, contribution_idr, contribution_pct) in enumerate(negative_factors[:3], 1):
                    priority = "üî¥" if contribution_pct >= 15.0 else ("üü°" if contribution_pct >= 7.0 else "üü¢")
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ —Ñ–∞–∫—Ç–æ—Ä–∞
                    if "–±—é–¥–∂–µ—Ç" in factor_name.lower():
                        rec_effect = contribution_idr * 0.8  # 80% –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                        recommendations.append(f"**{i}. {priority} –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–ª–∞–º–Ω—ã–π –±—é–¥–∂–µ—Ç**")
                        recommendations.append(f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç:** {_fmt_idr(rec_effect)}")
                        total_potential += rec_effect
                    elif "–≤—Ä–µ–º—è" in factor_name.lower():
                        rec_effect = contribution_idr * 0.6  # 60% –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                        recommendations.append(f"**{i}. {priority} –£—Å–∫–æ—Ä–∏—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã**")
                        recommendations.append(f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç:** {_fmt_idr(rec_effect)}")
                        total_potential += rec_effect
                    elif "—Ä–µ–π—Ç–∏–Ω–≥" in factor_name.lower():
                        rec_effect = contribution_idr * 0.7  # 70% –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                        recommendations.append(f"**{i}. {priority} –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞**")
                        recommendations.append(f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç:** {_fmt_idr(rec_effect)}")
                        total_potential += rec_effect
                    else:
                        rec_effect = contribution_idr * 0.5  # 50% –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        recommendations.append(f"**{i}. {priority} –ò—Å–ø—Ä–∞–≤–∏—Ç—å {factor_name.lower()}**")
                        recommendations.append(f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç:** {_fmt_idr(rec_effect)}")
                        total_potential += rec_effect
                
                for rec in recommendations:
                    day_lines.append(rec)
                
                day_lines.append("")
                day_lines.append("### üí∞ **–§–ò–ù–ê–ù–°–û–í–´–ô –ò–¢–û–ì**")
                recovery_pct = (total_potential / loss_amount * 100) if loss_amount > 0 else 0
                day_lines.append(f"- **–û–±—â–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è:** {_fmt_idr(total_potential)} ({recovery_pct:.0f}% –æ—Ç –ø–æ—Ç–µ—Ä—å)")
                day_lines.append("")
                
            except Exception as e:
                if REPORT_STRICT_MODE:
                    day_lines.append("### ‚ö†Ô∏è **ML –ê–ù–ê–õ–ò–ó –ù–ï–î–û–°–¢–£–ü–ï–ù**")
                    day_lines.append(f"–û—à–∏–±–∫–∞ ML –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
                    day_lines.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∏–ª–∏ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")
                    day_lines.append("")
            
            return day_lines
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–Ω–∏ —Å –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–æ–π
        for critical_date in critical_dates:
            day_analysis = _analyze_critical_day_improved(critical_date)
            lines.extend(day_analysis)
        
        # –û–±—â–∏–µ –≤—ã–≤–æ–¥—ã
        lines.append(f"üí∏ **–û–ë–©–ò–ï –ü–û–¢–ï–†–ò –û–¢ –í–°–ï–• –ö–†–ò–¢–ò–ß–ï–°–ö–ò–• –î–ù–ï–ô: {_fmt_idr(total_losses)}**")
        lines.append("")
        
        lines.append("üìä –û–ë–©–ò–ï –í–´–í–û–î–´")
        lines.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        lines.append(f"üìä –í—Å–µ–≥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –¥–Ω–µ–π: {len(critical_dates)} –∏–∑ {len(daily)} ({len(critical_dates)/len(daily)*100:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –¥–Ω–µ–π
        holiday_days = 0
        rainy_days = 0
        avg_loss = total_losses / len(critical_dates) if critical_dates else 0
        
        for critical_date in critical_dates:
            day_data = sub[sub["date"] == critical_date].iloc[0] if not sub[sub["date"] == critical_date].empty else None
            if day_data is not None:
                if int(day_data.get("is_holiday", 0)) == 1:
                    holiday_days += 1
                if float(day_data.get("rain", 0) or 0) >= 2.0:
                    rainy_days += 1
        
        lines.append(f"üïå –ü—Ä–∞–∑–¥–Ω–∏—á–Ω—ã—Ö –¥–Ω–µ–π: {holiday_days} ({holiday_days/len(critical_dates)*100:.0f}%)")
        lines.append(f"üåßÔ∏è –î–æ–∂–¥–ª–∏–≤—ã—Ö –¥–Ω–µ–π: ~{rainy_days} ({rainy_days/len(critical_dates)*100:.0f}%)")
        lines.append(f"üìà –°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–µ–Ω—å: {_fmt_idr(avg_loss)}")
        lines.append("")
        lines.append("üéØ –ü–†–ò–û–†–ò–¢–ï–¢–ù–ê–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:")
        lines.append("–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞–±–æ—Ç—ã –≤ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –∏ –¥–æ–∂–¥–ª–∏–≤—ã–µ –¥–Ω–∏")
        lines.append("")
        lines.append("üìã –ò–°–¢–û–ß–ù–ò–ö–ò –î–ê–ù–ù–´–•:")
        lines.append("- SQLite (grab_stats, gojek_stats) ‚Äî –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        lines.append("- Open-Meteo API ‚Äî –ø–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        lines.append("- Holidays cache ‚Äî –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ (–º—É—Å—É–ª—å–º–∞–Ω—Å–∫–∏–µ, –±–∞–ª–∏–π—Å–∫–∏–µ, –∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–∏–µ, –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ)")
        lines.append("- ML –º–æ–¥–µ–ª—å (Random Forest) ‚Äî SHAP –∞–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤")
        
        return "\\n".join(lines)
    
    except Exception:
        return "8. üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –î–ù–ò\\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–¥–µ–ª (–æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö)."
        mdl = model.named_steps["model"]
        X_pre = pre.transform(X)
        try:
            if background is not None and not background.empty:
                bg_pre = pre.transform(background[features])
                explainer = shap.TreeExplainer(mdl, data=bg_pre, feature_perturbation="interventional")
            else:
                explainer = shap.TreeExplainer(mdl, feature_perturbation="interventional")
            shap_values = explainer.shap_values(X_pre)
        except Exception:
            explainer = shap.TreeExplainer(mdl)
            shap_values = explainer.shap_values(X_pre)

        _, groups = _resolve_preprocessed_feature_groups(pre)
        # Exclude trivial features
        pat = [re.compile(r"^orders_count(?!.*conversion).*"), re.compile(r"^total_sales.*"), re.compile(r"^restaurant_id$")]
        def is_excluded(n: str) -> bool:
            return any(p.search(n) for p in pat)

        eng = get_engine()
        # Period baselines (–¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π)
        qg_all = pd.read_sql_query(
            "SELECT stat_date, ads_spend, ads_sales, cancelled_orders, offline_rate FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
            eng, params=(restaurant_id, start_str, end_str)
        )
        qj_all = pd.read_sql_query(
            "SELECT stat_date, ads_spend, ads_sales, accepting_time, preparation_time, delivery_time, close_time, cancelled_orders FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
            eng, params=(restaurant_id, start_str, end_str)
        )
        qg_all['stat_date'] = pd.to_datetime(qg_all['stat_date'], errors='coerce').dt.date
        qj_all['stat_date'] = pd.to_datetime(qj_all['stat_date'], errors='coerce').dt.date
        def _to_min_p(v):
            if v is None:
                return None
            s = str(v)
            parts = s.split(":")
            try:
                if len(parts) == 3:
                    h, m, sec = parts
                    return int(h) * 60 + int(m) + int(sec) / 60.0
            except Exception:
                pass
            try:
                return float(s)
            except Exception:
                return None
        def _safe_mean(series):
            s = pd.to_numeric(series, errors='coerce').dropna()
            return float(s.mean()) if not s.empty else None
        # Baselines
        spend_g_avg = _safe_mean(qg_all.get('ads_spend'))
        spend_j_avg = _safe_mean(qj_all.get('ads_spend'))
        roas_g_avg = None
        if not qg_all.empty:
            tmp = []
            for _, r in qg_all.iterrows():
                a, s_ = r.get('ads_spend'), r.get('ads_sales')
                try:
                    a = float(a); s_ = float(s_)
                    if a > 0:
                        tmp.append(s_ / a)
                except Exception:
                    pass
            roas_g_avg = float(np.mean(tmp)) if tmp else None
        roas_j_avg = None
        if not qj_all.empty:
            tmp = []
            for _, r in qj_all.iterrows():
                a, s_ = r.get('ads_spend'), r.get('ads_sales')
                try:
                    a = float(a); s_ = float(s_)
                    if a > 0:
                        tmp.append(s_ / a)
                except Exception:
                    pass
            roas_j_avg = float(np.mean(tmp)) if tmp else None
        canc_g_avg = _safe_mean(qg_all.get('cancelled_orders'))
        canc_j_avg = _safe_mean(qj_all.get('cancelled_orders'))
        off_g_avg = _safe_mean(qg_all.get('offline_rate'))
        prep_avg = _safe_mean(qj_all.get('preparation_time').apply(_to_min_p)) if 'preparation_time' in qj_all.columns else None
        accept_avg = _safe_mean(qj_all.get('accepting_time').apply(_to_min_p)) if 'accepting_time' in qj_all.columns else None
        deliv_avg = _safe_mean(qj_all.get('delivery_time').apply(_to_min_p)) if 'delivery_time' in qj_all.columns else None

        for d in critical_dates:
            day_mask = sub["date"].dt.normalize() == d
            idxs = np.where(day_mask.values)[0]
            if len(idxs) == 0:
                continue
            # Aggregate contributions over all rows of that date (should typically be 1 per date)
            contrib_sum: Dict[str, float] = {}
            for i in idxs:
                for feat, cols in groups.items():
                    if is_excluded(feat):
                        continue
                    if not cols:
                        continue
                    val = float(np.sum(shap_values[i, cols]))
                    contrib_sum[feat] = contrib_sum.get(feat, 0.0) + val
            # Select significant factors by |impact|: cover ~95% cumulatively and include any with share >=1%
            contrib_sorted = sorted(contrib_sum.items(), key=lambda x: abs(x[1]), reverse=True)
            total_abs = sum(abs(v) for v in contrib_sum.values()) or 1.0
            selected = []
            cum = 0.0
            for feat, val in contrib_sorted:
                share = abs(val) / total_abs
                if share >= 0.01 or cum < 0.95:
                    selected.append((feat, val))
                    cum += share
                else:
                    break

            # Group shares
            group_shares: Dict[str, float] = {}
            for feat, val in contrib_sum.items():
                cat = _categorize_feature(feat)
                group_shares[cat] = group_shares.get(cat, 0.0) + abs(val)
            for k in list(group_shares.keys()):
                group_shares[k] = round(100.0 * group_shares[k] / total_abs, 1)

            # Day-level raw details from stats
            ds = str(d.date())
            qg = pd.read_sql_query(
                "SELECT sales, orders, ads_spend, ads_sales, offline_rate, cancelled_orders FROM grab_stats WHERE restaurant_id=? AND stat_date=?",
                eng, params=(restaurant_id, ds)
            )
            qj = pd.read_sql_query(
                "SELECT sales, orders, ads_spend, ads_sales, accepting_time, preparation_time, delivery_time, close_time, cancelled_orders FROM gojek_stats WHERE restaurant_id=? AND stat_date=?",
                eng, params=(restaurant_id, ds)
            )
            grab_off_mins = float(qg.iloc[0]["offline_rate"]) if (not qg.empty and pd.notna(qg.iloc[0]["offline_rate"])) else None
            gojek_close = str(qj.iloc[0]["close_time"]) if (not qj.empty and pd.notna(qj.iloc[0]["close_time"])) else ""
            # close_time may be HH:MM:SS
            def _hms_close(s: str) -> str:
                parts = s.split(":") if s else []
                try:
                    if len(parts) == 3:
                        h, m, sec = parts
                        return f"{int(h)}:{int(m):02d}:{int(sec):02d}"
                except Exception:
                    pass
                return "‚Äî"

            # Weather/holiday from dataset row (first match of the date)
            row = sub.loc[day_mask].iloc[0]
            rain = float(row.get("rain")) if pd.notna(row.get("rain")) else None
            temp = float(row.get("temp")) if pd.notna(row.get("temp")) else None
            wind = float(row.get("wind")) if pd.notna(row.get("wind")) else None
            hum = float(row.get("humidity")) if pd.notna(row.get("humidity")) else None
            is_hol = int(row.get("is_holiday")) if pd.notna(row.get("is_holiday")) else 0
            total_sales_day = float(daily.loc[daily["date"] == d, "total_sales"].iloc[0])
            delta_pct = ((total_sales_day - med) / med * 100.0) if med else None
            delta_idr = max(med - total_sales_day, 0.0) if med else 0.0
            expected_idr = _expected_baseline_for_day(daily, d)
            drop_idr = max(0.0, expected_idr - total_sales_day)

            lines.append(f"üìâ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –î–ï–ù–¨: {ds} (–≤—ã—Ä—É—á–∫–∞: {_fmt_idr(total_sales_day)}; –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∫ –º–µ–¥–∏–∞–Ω–µ: {_fmt_pct(delta_pct)})")
            lines.append("‚Äî" * 72)
            # Compute business-oriented factor sets (threshold 3%)
            def _share(v: float) -> float:
                return round(100.0 * abs(v) / total_abs, 1)
            def _is_technical(name: str) -> bool:
                n = name.lower()
                return ('lag' in n) or ('rolling' in n)
            sig_all = [(f, v, _share(v)) for f, v in selected if _share(v) >= 3.0]
            sig = [(f, v, s) for f, v, s in sig_all if not _is_technical(f)] or sig_all
            neg = [(f, v, s) for f, v, s in sig if v < 0]
            pos = [(f, v, s) for f, v, s in sig if v > 0]
            neg = sorted(neg, key=lambda x: x[2], reverse=True)[:5]
            pos = sorted(pos, key=lambda x: x[2], reverse=True)[:2]

            # Compute monetary effect for negative factors and deduplicate
            neg_total_abs = sum(abs(v) for f, v in contrib_sum.items() if v < 0) or 1.0
            factor_rows_neg: list[tuple[str, float, int]] = []
            seen_canon: set[str] = set()
            def _canon(name: str) -> str:
                n = name.lower()
                n = n.replace("preparation_time_mean","preparation_time").replace("accepting_time_mean","accepting_time").replace("delivery_time_mean","delivery_time")
                return n
            for f, v, s in neg:
                canon = _canon(f)
                if canon in seen_canon:
                    continue
                seen_canon.add(canon)
                money = round((abs(v) / neg_total_abs) * drop_idr)
                if s < 5.0 and money < 50000:
                    continue
                factor_rows_neg.append((f, s, money))

            # Day-level metrics snapshot for comments
            # Build baselines already computed above: roas_g_avg, roas_j_avg, prep/accept/deliv avg, etc.
            day_roas_g = None; day_roas_j = None; day_spend_g = None; day_spend_j = None
            day_ads_sales_g = None; day_ads_sales_j = None
            if not qg.empty:
                gs = qg.iloc[0]
                day_spend_g = float(gs.get('ads_spend')) if pd.notna(gs.get('ads_spend')) else None
                day_roas_g = (float(gs.get('ads_sales')) / float(gs.get('ads_spend'))) if (pd.notna(gs.get('ads_spend')) and float(gs.get('ads_spend'))>0) else None
                day_ads_sales_g = float(gs.get('ads_sales')) if pd.notna(gs.get('ads_sales')) else None
            if not qj.empty:
                js = qj.iloc[0]
                day_spend_j = float(js.get('ads_spend')) if pd.notna(js.get('ads_spend')) else None
                day_roas_j = (float(js.get('ads_sales')) / float(js.get('ads_spend'))) if (pd.notna(js.get('ads_spend')) and float(js.get('ads_spend'))>0) else None
                day_ads_sales_j = float(js.get('ads_sales')) if pd.notna(js.get('ads_sales')) else None
            d_prep = _to_min_p(qj.iloc[0].get('preparation_time')) if not qj.empty else None
            d_acc = _to_min_p(qj.iloc[0].get('accepting_time')) if not qj.empty else None
            d_del = _to_min_p(qj.iloc[0].get('delivery_time')) if not qj.empty else None

            def _comment_for(feat_name: str, is_positive: bool) -> str:
                n = feat_name.lower()
                # Marketing
                if 'roas' in n:
                    # choose platform
                    if 'grab' in n and day_roas_g is not None and roas_g_avg is not None:
                        if roas_g_avg > 0:
                            diff = (day_roas_g - roas_g_avg) / roas_g_avg * 100.0
                            trend = '–≤—ã—Ä–æ—Å' if diff > 0 else ('—É–ø–∞–ª' if diff < 0 else '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π')
                            if trend == '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π':
                                return '—Ä–µ–∫–ª–∞–º–∞ GRAB ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π'
                            return f"ROAS GRAB {trend} –Ω–∞ {abs(diff):.0f}% ({day_roas_g:.2f}x –ø—Ä–æ—Ç–∏–≤ {roas_g_avg:.2f}x)"
                        return "—Ä–µ–∫–ª–∞–º–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å GRAB ‚Äî –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö"
                    if 'gojek' in n and day_roas_j is not None and roas_j_avg is not None:
                        if roas_j_avg > 0:
                            diff = (day_roas_j - roas_j_avg) / roas_j_avg * 100.0
                            trend = '–≤—ã—Ä–æ—Å' if diff > 0 else ('—É–ø–∞–ª' if diff < 0 else '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π')
                            if trend == '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π':
                                return '—Ä–µ–∫–ª–∞–º–∞ GOJEK ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π'
                            return f"ROAS GOJEK {trend} –Ω–∞ {abs(diff):.0f}% ({day_roas_j:.2f}x –ø—Ä–æ—Ç–∏–≤ {roas_j_avg:.2f}x)"
                        return "—Ä–µ–∫–ª–∞–º–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å GOJEK ‚Äî –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö"
                    return "—Ä–µ–∫–ª–∞–º–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∏–∂–µ –Ω–æ—Ä–º—ã" if not is_positive else "—Ä–µ–∫–ª–∞–º–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—à–µ –Ω–æ—Ä–º—ã"
                if 'ads_spend' in n or 'budget' in n:
                    if 'grab' in n and day_spend_g is not None and spend_g_avg is not None:
                        if spend_g_avg > 0:
                            diff = (day_spend_g - spend_g_avg) / spend_g_avg * 100.0
                            trend = '–≤—ã—à–µ' if diff > 0 else ('–Ω–∏–∂–µ' if diff < 0 else '–Ω–∞ —É—Ä–æ–≤–Ω–µ')
                            if trend == '–Ω–∞ —É—Ä–æ–≤–Ω–µ':
                                return '–±—é–¥–∂–µ—Ç GRAB ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π'
                            return f"–±—é–¥–∂–µ—Ç GRAB {trend} —Å—Ä–µ–¥–Ω–µ–≥–æ –Ω–∞ {abs(diff):.0f}% ({_fmt_idr(day_spend_g)} –ø—Ä–æ—Ç–∏–≤ {_fmt_idr(spend_g_avg)})"
                        return "–±—é–¥–∂–µ—Ç GRAB ‚Äî –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö"
                    if 'gojek' in n and day_spend_j is not None and spend_j_avg is not None:
                        if spend_j_avg > 0:
                            diff = (day_spend_j - spend_j_avg) / spend_j_avg * 100.0
                            trend = '–≤—ã—à–µ' if diff > 0 else ('–Ω–∏–∂–µ' if diff < 0 else '–Ω–∞ —É—Ä–æ–≤–Ω–µ')
                            if trend == '–Ω–∞ —É—Ä–æ–≤–Ω–µ':
                                return '–±—é–¥–∂–µ—Ç GOJEK ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π'
                            return f"–±—é–¥–∂–µ—Ç GOJEK {trend} —Å—Ä–µ–¥–Ω–µ–≥–æ –Ω–∞ {abs(diff):.0f}% ({_fmt_idr(day_spend_j)} –ø—Ä–æ—Ç–∏–≤ {_fmt_idr(spend_j_avg)})"
                        return "–±—é–¥–∂–µ—Ç GOJEK ‚Äî –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö"
                    return "–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∫–ª–∞–º–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"
                # Operations
                if 'preparation_time' in n:
                    if d_prep is not None and prep_avg is not None and prep_avg > 0:
                        diff = (d_prep - prep_avg) / prep_avg * 100.0
                        if abs(diff) < 0.5:
                            return "–≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                        trend = '–Ω–∏–∂–µ –Ω–æ—Ä–º—ã' if diff < 0 else '–≤—ã—à–µ –Ω–æ—Ä–º—ã'
                        return f"–≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è {trend} –Ω–∞ {abs(diff):.0f}% ({d_prep:.1f} –ø—Ä–æ—Ç–∏–≤ {prep_avg:.1f} –º–∏–Ω)"
                    return "—Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è"
                if 'accepting_time' in n:
                    if d_acc is not None and accept_avg is not None and accept_avg > 0:
                        diff = (d_acc - accept_avg) / accept_avg * 100.0
                        if abs(diff) < 0.5:
                            return "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                        trend = '–±—ã—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ' if diff < 0 else '–¥–æ–ª—å—à–µ –æ–±—ã—á–Ω–æ–≥–æ'
                        return f"–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ {trend} –Ω–∞ {abs(diff):.0f}% ({d_acc:.1f} –ø—Ä–æ—Ç–∏–≤ {accept_avg:.1f} –º–∏–Ω)"
                    return "—Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"
                if 'delivery_time' in n:
                    if d_del is not None and deliv_avg is not None and deliv_avg > 0:
                        diff = (d_del - deliv_avg) / deliv_avg * 100.0
                        if abs(diff) < 0.5:
                            return "–¥–æ—Å—Ç–∞–≤–∫–∞ ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                        trend = '–±—ã—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ' if diff < 0 else '–¥–æ–ª—å—à–µ –æ–±—ã—á–Ω–æ–≥–æ'
                        return f"–¥–æ—Å—Ç–∞–≤–∫–∞ {trend} –Ω–∞ {abs(diff):.0f}% ({d_del:.1f} –ø—Ä–æ—Ç–∏–≤ {deliv_avg:.1f} –º–∏–Ω)"
                    return "—Å–∫–æ—Ä–æ—Å—Ç—å –¥–æ—Å—Ç–∞–≤–∫–∏"
                if 'offline' in n or 'outage' in n:
                    return "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –±—ã–ª–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–æ—Ñ—Ñ–ª–∞–π–Ω)"
                # External
                if 'rain' in n:
                    return "–¥–æ–∂–¥—å —Å–Ω–∏–∑–∏–ª —Å–ø—Ä–æ—Å" if not is_positive else "–ø–æ–≥–æ–¥–∞ –±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–∞"
                if 'day_of_week' in n or 'weekend' in n:
                    return "—Å–ª–∞–±—ã–π –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏" if not is_positive else "—Å–∏–ª—å–Ω—ã–π –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏"
                # –£–±–∏—Ä–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É/–≤–ª–∞–∂–Ω–æ—Å—Ç—å/–≤–µ—Ç–µ—Ä –∏–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –¥–ª—è –±–∏–∑–Ω–µ—Å–∞
                if 'humidity' in n or 'wind' in n or 'temp' in n:
                    return "–≤–Ω–µ—à–Ω–∏–µ —É—Å–ª–æ–≤–∏—è –ø–æ–≤–ª–∏—è–ª–∏"
                if 'rating' in n:
                    return "—Ä–µ–π—Ç–∏–Ω–≥ –ø–æ–≤–ª–∏—è–ª –Ω–∞ —Å–ø—Ä–æ—Å"
                return "–≤–ª–∏—è—é—â–∏–π —Ñ–∞–∫—Ç–æ—Ä –ø–µ—Ä–∏–æ–¥–∞"

            # –ï–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –±–ª–æ–∫–∞ "–§–∞–∫—Ç"/"–ì–ª–∞–≤–Ω—ã–µ –¥—Ä–∞–π–≤–µ—Ä—ã"
            if grab_off_mins and grab_off_mins > 0:
                pass

            # Marketing block (per-channel deltas vs period average)
            def _pct_delta(val: Optional[float], avg: Optional[float]) -> Optional[float]:
                try:
                    if val is None or avg is None or float(avg) == 0.0:
                        return None
                    return (float(val) - float(avg)) / float(avg) * 100.0
                except Exception:
                    return None
            avg_ads_sales_g = _safe_mean(qg_all.get('ads_sales')) if 'ads_sales' in qg_all.columns else None
            avg_ads_sales_j = _safe_mean(qj_all.get('ads_sales')) if 'ads_sales' in qj_all.columns else None
            spend_g_delta = _pct_delta(day_spend_g, spend_g_avg)
            spend_j_delta = _pct_delta(day_spend_j, spend_j_avg)
            sales_g_delta = _pct_delta(day_ads_sales_g, avg_ads_sales_g)
            sales_j_delta = _pct_delta(day_ads_sales_j, avg_ads_sales_j)
            # Headline driver examples
            mk_headlines = []
            if spend_j_delta is not None and sales_j_delta is not None and spend_j_delta > 0 and sales_j_delta < 0:
                mk_headlines.append(f"GOJEK Ads: –±—é–¥–∂–µ—Ç +{abs(spend_j_delta):.1f}% –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –ø—Ä–æ–¥–∞–∂ {abs(sales_j_delta):.1f}%")
            if spend_g_delta is not None and sales_g_delta is not None and spend_g_delta < 0 and sales_g_delta < 0:
                mk_headlines.append(f"GRAB Ads: –±—é–¥–∂–µ—Ç ‚àí{abs(spend_g_delta):.1f}% –∏ –ø—Ä–æ–¥–∞–∂–∏ ‚àí{abs(sales_g_delta):.1f}% (—Å–Ω–∏–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)")
            if spend_j_delta is not None and sales_j_delta is not None and spend_j_delta < 0 and sales_j_delta < 0:
                mk_headlines.append(f"GOJEK Ads: –±—é–¥–∂–µ—Ç ‚àí{abs(spend_j_delta):.1f}% –∏ –ø—Ä–æ–¥–∞–∂–∏ ‚àí{abs(sales_j_delta):.1f}% (—Å–Ω–∏–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)")
            if mk_headlines:
                for hl in mk_headlines[:2]:
                    lines.append(f"- {hl}.")
                lines.append("")
            # Marketing table
            lines.append("–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥:")
            lines.append("| –ö–∞–Ω–∞–ª | Ads Spend | Ads Sales | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |")
            lines.append("|---|---:|---:|---|")
            def _eff_label(spend_d: Optional[float], sales_d: Optional[float]) -> str:
                if spend_d is None or sales_d is None:
                    return "‚Äî"
                if spend_d > 0 and sales_d < 0:
                    return "–±—é–¥–∂–µ—Ç ‚Üë, –ø—Ä–æ–¥–∞–∂–∏ ‚Üì ‚Üí —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–ª–æ—Ö–∞—è"
                if spend_d < 0 and sales_d > 0:
                    return "–±—é–¥–∂–µ—Ç ‚Üì, –ø—Ä–æ–¥–∞–∂–∏ ‚Üë ‚Üí —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ö–æ—Ä–æ—à–∞—è"
                if spend_d < 0 and sales_d < 0:
                    return "–±—é–¥–∂–µ—Ç ‚Üì, –ø—Ä–æ–¥–∞–∂–∏ ‚Üì"
                if spend_d > 0 and sales_d > 0:
                    return "–±—é–¥–∂–µ—Ç ‚Üë, –ø—Ä–æ–¥–∞–∂–∏ ‚Üë"
                return "‚Äî"
            def _mk_row(label: str, spend_val, spend_d, sales_val, sales_d) -> str:
                cmt = []
                if spend_d is not None:
                    cmt.append("–±—é–¥–∂–µ—Ç ‚Üë" + f"{abs(spend_d):.1f}%" if spend_d > 0 else "–±—é–¥–∂–µ—Ç ‚Üì" + f"{abs(spend_d):.1f}%")
                if sales_d is not None:
                    cmt.append("–ø—Ä–æ–¥–∞–∂–∏ ‚Üë" + f"{abs(sales_d):.1f}%" if sales_d > 0 else "–ø—Ä–æ–¥–∞–∂–∏ ‚Üì" + f"{abs(sales_d):.1f}%")
                cmt.append(_eff_label(spend_d, sales_d))
                comment = ", ".join([p for p in cmt if p]) if cmt else "‚Äî"
                return f"| {label} | {_fmt_idr(spend_val)} ({_fmt_pct(spend_d)}) | {_fmt_idr(sales_val)} ({_fmt_pct(sales_d)}) | {comment} |"
            lines.append(_mk_row("GOJEK", day_spend_j, spend_j_delta, day_ads_sales_j, sales_j_delta))
            lines.append(_mk_row("GRAB", day_spend_g, spend_g_delta, day_ads_sales_g, sales_g_delta))
            lines.append("")

            # External factors (unified rain classification)
            lines.append("–í–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã:")
            # Try to resolve holiday name
            holiday_name = None
            try:
                hol_df = load_holidays_df(start_str, end_str)
                if not hol_df.empty:
                    name_row = hol_df[hol_df["date"] == pd.to_datetime(ds)].head(1)
                    if not name_row.empty:
                        holiday_name = str(name_row.iloc[0].get("holiday_name") or "").strip()
            except Exception:
                holiday_name = None
            hol_label = f"–¥–∞ ({holiday_name})" if (is_hol and holiday_name) else ("–¥–∞" if is_hol else "–Ω–µ—Ç")
            lines.append(f"- –ü—Ä–∞–∑–¥–Ω–∏–∫: {hol_label}{(' ‚Äî –≤–æ–∑–º–æ–∂–Ω–∞ –Ω–∏–∑–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫—É—Ä—å–µ—Ä–æ–≤') if is_hol else ''}")
            def _rain_label(val: Optional[float]) -> str:
                try:
                    r = float(val) if val is not None else 0.0
                except Exception:
                    r = 0.0
                if r <= 0.0:
                    return "–Ω–µ –±—ã–ª–æ"
                if r < 10.0:
                    return "—Å–ª–∞–±—ã–π"
                return "—Å–∏–ª—å–Ω—ã–π"
            lines.append(f"- –î–æ–∂–¥—å: {_rain_label(rain)}")
            # Cancellations snapshot
            canc_g_day = int(qg.iloc[0]["cancelled_orders"]) if (not qg.empty and pd.notna(qg.iloc[0]["cancelled_orders"])) else 0
            canc_j_day = int(qj.iloc[0]["cancelled_orders"]) if (not qj.empty and pd.notna(qj.iloc[0]["cancelled_orders"])) else 0
            lines.append(f"- –û—Ç–º–µ–Ω—ã: GRAB {canc_g_day}; GOJEK {canc_j_day}")
            lines.append("")

            # Unified causes (marketing + ML + externals) ‚Äî marketing-friendly wording
            causes: list[str] = []
            if spend_j_delta is not None and sales_j_delta is not None and spend_j_delta > 0 and sales_j_delta < 0:
                causes.append(f"—Ä–µ–∫–ª–∞–º–∞ GOJEK –æ–∫–∞–∑–∞–ª–∞—Å—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π: –±—é–¥–∂–µ—Ç —É–≤–µ–ª–∏—á–∏–ª–∏ (+{abs(spend_j_delta):.1f}%), –∞ –ø—Ä–æ–¥–∞–∂–∏ —É–ø–∞–ª–∏ (‚àí{abs(sales_j_delta):.1f}%)")
            if spend_g_delta is not None and sales_g_delta is not None and spend_g_delta < 0 and sales_g_delta < 0:
                causes.append(f"—Å–Ω–∏–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ GRAB: –±—é–¥–∂–µ—Ç —Å–Ω–∏–∑–∏–ª–∏ (‚àí{abs(spend_g_delta):.1f}%), –ø—Ä–æ–¥–∞–∂–∏ —Ç–∞–∫–∂–µ –ø—Ä–æ—Å–µ–ª–∏ (‚àí{abs(sales_g_delta):.1f}%)")
            if spend_j_delta is not None and sales_j_delta is not None and spend_j_delta < 0 and sales_j_delta < 0:
                causes.append(f"—Å–Ω–∏–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ GOJEK: –±—é–¥–∂–µ—Ç —Å–Ω–∏–∑–∏–ª–∏ (‚àí{abs(spend_j_delta):.1f}%), –ø—Ä–æ–¥–∞–∂–∏ —Ç–∞–∫–∂–µ –ø—Ä–æ—Å–µ–ª–∏ (‚àí{abs(sales_j_delta):.1f}%)")
            # External chain: holiday -> couriers -> cancels -> sales
            canc_g_avg = _safe_mean(qg_all.get('cancelled_orders'))
            canc_j_avg = _safe_mean(qj_all.get('cancelled_orders'))
            canc_up = ((canc_g_day and canc_g_avg and canc_g_day > canc_g_avg) or (canc_j_day and canc_j_avg and canc_j_day > canc_j_avg))
            if is_hol and canc_up:
                if holiday_name:
                    causes.append(f"{holiday_name}: –º–µ–Ω—å—à–µ –∫—É—Ä—å–µ—Ä–æ–≤ ‚Üí –±–æ–ª—å—à–µ –æ—Ç–º–µ–Ω ‚Üí –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂")
                else:
                    causes.append("–ø—Ä–∞–∑–¥–Ω–∏–∫: –º–µ–Ω—å—à–µ –∫—É—Ä—å–µ—Ä–æ–≤ ‚Üí –±–æ–ª—å—à–µ –æ—Ç–º–µ–Ω ‚Üí –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂")
            elif is_hol:
                causes.append("–ø—Ä–∞–∑–¥–Ω–∏–∫ —Å–Ω–∏–∑–∏–ª –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫—É—Ä—å–µ—Ä–æ–≤ –∏ —Å–ø—Ä–æ—Å")
            # –î–æ–∂–¥—å: —Å–∏–ª—å–Ω—ã–π ‚Äî –Ω–µ–≥–∞—Ç–∏–≤, —Å–ª–∞–±—ã–π ‚Äî –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç (–Ω–µ –≤–∫–ª—é—á–∞–µ–º –≤ –ø—Ä–∏—á–∏–Ω—ã)
            try:
                r = float(rain) if rain is not None else 0.0
            except Exception:
                r = 0.0
            if r >= 10.0:
                causes.append("—Å–∏–ª—å–Ω—ã–π –¥–æ–∂–¥—å: –º–µ–Ω—å—à–µ –∫—É—Ä—å–µ—Ä–æ–≤ –∏ –±–æ–ª—å—à–µ –æ—Ç–º–µ–Ω ‚Üí –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂")
            # Add top ML factors as plain language without duplicates (collapse prep time variants)
            added_ml = 0
            used_prefixes = set()
            for f, v, s in neg:
                base = f.lower().replace("preparation_time_mean", "preparation_time").replace("ops_preparation_time_gojek", "preparation_time").replace("ops_preparation_time_grab", "preparation_time")
                if base in used_prefixes:
                    continue
                used_prefixes.add(base)
                cmt = _comment_for(f, False)
                label = _pretty_feature_name(f)
                causes.append(f"{label}: {cmt}")
                added_ml += 1
                if added_ml >= 2:
                    break
            # Deduplicate while preserving order
            seen = set()
            causes_unique = []
            for c in causes:
                key = c.lower()
                if key in seen:
                    continue
                seen.add(key)
                causes_unique.append(c)
            if causes_unique:
                lines.append("–ü—Ä–∏—á–∏–Ω—ã:")
                for c in causes_unique:
                    lines.append(f"- {c}.")
                lines.append("")

            # Short summary and factor tables
            lines.append("–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ:")
            lines.append(f"- –ü—Ä–æ—Å–∞–¥–∫–∞: ‚àí{_fmt_idr(drop_idr)} ({_fmt_pct(delta_pct)} –∫ –º–µ–¥–∏–∞–Ω–µ/–æ–∂–∏–¥–∞–Ω–∏—é).")
            if factor_rows_neg:
                topn = ", ".join([f"{_pretty_feature_name(f)} (‚àí{_fmt_idr(m)})" for f, _, m in factor_rows_neg[:2]])
                lines.append(f"- –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: {topn}.")
            lines.append("")
            if factor_rows_neg:
                lines.append("–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (–¢–û–ü‚Äë5):")
                lines.append("| –§–∞–∫—Ç–æ—Ä | –í–∫–ª–∞–¥ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |")
                lines.append("|---|---:|---|")
                for f, s, money in sorted(factor_rows_neg, key=lambda x: (x[2], x[1]), reverse=True)[:5]:
                    fname = f.lower()
                    if ('temp' in fname) or ('temperature' in fname) or ('humidity' in fname) or ('wind' in fname):
                        continue
                    lines.append(f"| {_pretty_feature_name(f)} | ‚àí{_fmt_idr(money)} ({s}%) | {_comment_for(f, False)} |")
                lines.append("")
            if pos:
                lines.append("–ß—Ç–æ –ø–æ–º–æ–≥–ª–æ (–¥–æ 2 —Ñ–∞–∫—Ç–æ—Ä–æ–≤):")
                lines.append("| –§–∞–∫—Ç–æ—Ä | –í–ª–∏—è–Ω–∏–µ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |")
                lines.append("|---|---:|---|")
                for f, v, s in pos:
                    lines.append(f"| {_pretty_feature_name(f)} | {s}% | {_comment_for(f, True)} |")
                lines.append("")

            # Priorities helpers
            def _priority_tag(share: float) -> str:
                return "üî¥" if share >= 15.0 else ("üü†" if share >= 7.0 else "üü¢")
            def _confidence(share: float) -> str:
                return "High" if share >= 15.0 else ("Medium" if share >= 8.0 else "Low")

            # Build full lists by category (significant negative >=3%)
            neg_sorted = sorted(neg, key=lambda x: x[2], reverse=True)
            pos_sorted = sorted(pos, key=lambda x: x[2], reverse=True)
            cat_to_neg: Dict[str, list] = {}
            for f, v, s in neg_sorted:
                cat_to_neg.setdefault(_categorize_feature(f), []).append((f, s))
            cat_to_pos: Dict[str, list] = {}
            for f, v, s in pos_sorted:
                cat_to_pos.setdefault(_categorize_feature(f), []).append((f, s))

            # Narrative 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            top_cats = sorted([(c, sum(s for _, s in fs)) for c, fs in cat_to_neg.items()], key=lambda x: x[1], reverse=True)
            if top_cats:
                label = {"Marketing": "—Ä–µ–∫–ª–∞–º–∞", "Operations": "–æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –∫—É—Ö–Ω–µ/–¥–æ—Å—Ç–∞–≤–∫–µ", "External": "–≤–Ω–µ—à–Ω–∏–µ —É—Å–ª–æ–≤–∏—è (–ø–æ–≥–æ–¥–∞/–∫–∞–ª–µ–Ω–¥–∞—Ä—å)", "Quality": "–∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞"}
                cats = [label.get(c, c) for c, _ in top_cats[:2]]
                if len(cats) >= 2:
                    lines.append(f"–≠—Ç–æ—Ç –¥–µ–Ω—å –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è –∏–∑‚Äë–∑–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ {cats[0]} –∏ {cats[1]}.")
                else:
                    lines.append(f"–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ –ø—Ä–æ—Å–∞–¥–∫–∏ ‚Äî {cats[0]}.")
                lines.append("")

            # –£–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –∫—Ä–∞—Ç–∫—É—é –ø–æ–º–µ—Ç–∫—É –æ–± —É—Å–ª–æ–≤–∏—è—Ö
            rain_share = _share(contrib_sum.get('rain', 0.0)) if 'rain' in contrib_sum else 0.0
            hol_share = _share(contrib_sum.get('is_holiday', 0.0)) if 'is_holiday' in contrib_sum else 0.0
            lines.append(f"‚Ä¢ –£—Å–ª–æ–≤–∏—è: –¥–æ–∂–¥—å ‚Äî {_rain_label(rain)}; –ø—Ä–∞–∑–¥–Ω–∏–∫ ‚Äî {'–¥–∞' if is_hol else '–Ω–µ—Ç'}")
            lines.append("")

            # –ß—Ç–æ —Å–º—è–≥—á–∞–ª–æ
            if pos_sorted:
                lines.append("‚úÖ –ß—Ç–æ —Å–º—è–≥—á–∞–ª–æ:")
                added = 0
                for f, v, s in pos_sorted:
                    cmt = _comment_for(f, True)
                    if '–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π' in cmt:
                        continue
                    lines.append(f"‚Ä¢ {_pretty_feature_name(f)} (+{s}%): {cmt}")
                    added += 1
                    if added >= 2:
                        break
                lines.append("")

            # Evidence lines are summarized in comments; skip protocol

            # What-if: –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ä—ã—á–∞–≥–∏ –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
            try:
                row_idx = idxs[0]
                xrow = X.iloc[[row_idx]].copy(deep=True)
                for col in ["preparation_time_mean", "accepting_time_mean", "delivery_time_mean"]:
                    if col in xrow.columns and pd.notna(xrow.iloc[0][col]):
                        xrow.loc[xrow.index[0], col] = max(0.0, float(xrow.iloc[0][col]) * 0.9)
                if "outage_offline_rate_grab" in xrow.columns and pd.notna(xrow.iloc[0]["outage_offline_rate_grab"]):
                    xrow.loc[xrow.index[0], "outage_offline_rate_grab"] = 0.0
                if "ads_spend_total" in xrow.columns and pd.notna(xrow.iloc[0]["ads_spend_total"]):
                    xrow.loc[xrow.index[0], "ads_spend_total"] = float(xrow.iloc[0]["ads_spend_total"]) * 1.1
                uplift = float(model.predict(xrow)[0] - model.predict(X.iloc[[row_idx]])[0])
                # Individual levers
                base_pred = float(model.predict(X.iloc[[row_idx]])[0])
                # SLA only
                x_sla = X.iloc[[row_idx]].copy(deep=True)
                for col in ["preparation_time_mean", "accepting_time_mean", "delivery_time_mean"]:
                    if col in x_sla.columns and pd.notna(x_sla.iloc[0][col]):
                        x_sla.loc[x_sla.index[0], col] = max(0.0, float(x_sla.iloc[0][col]) * 0.9)
                uplift_sla = float(model.predict(x_sla)[0] - base_pred)
                # Budget only
                x_bud = X.iloc[[row_idx]].copy(deep=True)
                if "ads_spend_total" in x_bud.columns and pd.notna(x_bud.iloc[0].get("ads_spend_total")):
                    x_bud.loc[x_bud.index[0], "ads_spend_total"] = float(x_bud.iloc[0]["ads_spend_total"]) * 1.1
                else:
                    if "mkt_ads_spend_grab" in x_bud.columns and pd.notna(x_bud.iloc[0].get("mkt_ads_spend_grab")):
                        x_bud.loc[x_bud.index[0], "mkt_ads_spend_grab"] = float(x_bud.iloc[0]["mkt_ads_spend_grab"]) * 1.1
                    if "mkt_ads_spend_gojek" in x_bud.columns and pd.notna(x_bud.iloc[0].get("mkt_ads_spend_gojek")):
                        x_bud.loc[x_bud.index[0], "mkt_ads_spend_gojek"] = float(x_bud.iloc[0]["mkt_ads_spend_gojek"]) * 1.1
                uplift_bud = float(model.predict(x_bud)[0] - base_pred)
                # Offline only
                x_off = X.iloc[[row_idx]].copy(deep=True)
                for col in ["outage_offline_rate_grab", "offline_rate_grab"]:
                    if col in x_off.columns and pd.notna(x_off.iloc[0].get(col)):
                        x_off.loc[x_off.index[0], col] = 0.0
                uplift_off = float(model.predict(x_off)[0] - base_pred)

                # Confidence per lever (by category contributions)
                cat_contribs = {k: group_shares.get(k, 0.0) for k in ["Marketing", "Operations", "External"]}
                conf_mark = _confidence(cat_contribs.get("Marketing", 0.0))
                conf_ops = _confidence(cat_contribs.get("Operations", 0.0))
                conf_off = _confidence(10.0 if (grab_off_mins and grab_off_mins > 0) else 0.0)
                lines.append("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                lines.append(f"üî¥ –°—Ä–æ—á–Ω–æ ‚Äî –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–∏—Ç—å –±—é–¥–∂–µ—Ç –Ω–∞ –∫–∞–º–ø–∞–Ω–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π –æ—Ç–¥–∞—á–µ–π (+10%): ‚âà {_fmt_idr(uplift_bud)} ({conf_mark})")
                lines.append(f"üü† –í–∞–∂–Ω–æ ‚Äî —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è/–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω–∞ 10%: ‚âà {_fmt_idr(uplift_sla)} ({conf_ops})")
                lines.append(f"üü¢ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ ‚Äî –∏—Å–∫–ª—é—á–∏—Ç—å –æ—Ñ—Ñ–ª–∞–π–Ω –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö: ‚âà {_fmt_idr(uplift_off)} ({conf_off})")
                lines.append("")
                lines.append(f"üí∞ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: ~{_fmt_idr(uplift)}")
                lines.append(f"üí∞ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {_fmt_idr(min_pot)} ‚Äî {_fmt_idr(max_pot)} (–±–∞–∑–∞: {_fmt_idr(base_pot)})")
                lines.append(f"üìà –ü—Ä–æ–≥–Ω–æ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (7 –¥–Ω–µ–π): {_fmt_idr(min_pot*7)} ‚Äî {_fmt_idr(max_pot*7)}")
            except Exception:
                pass
            lines.append("")

        # –°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø–µ—Ä–∏–æ–¥—É (–±–µ–∑ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏)
        lines.append("–°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø–µ—Ä–∏–æ–¥—É:")
        sub['heavy_rain'] = (sub['rain'].fillna(0.0) >= 10.0).astype(int)
        by_rain = sub.groupby('heavy_rain')['total_sales'].mean().to_dict()
        if 0 in by_rain:
            dr = (by_rain.get(1, by_rain[0]) - by_rain[0]) / (by_rain[0] or 1.0) * 100.0
            lines.append(f"  ‚Ä¢ üåßÔ∏è –î–æ–∂–¥—å: {_fmt_pct(dr)}")
        by_h = sub.groupby(sub['is_holiday'].fillna(0).astype(int))['total_sales'].mean().to_dict()
        if 0 in by_h:
            dh = (by_h.get(1, by_h[0]) - by_h[0]) / (by_h[0] or 1.0) * 100.0
            lines.append(f"  ‚Ä¢ üéå –ü—Ä–∞–∑–¥–Ω–∏–∫–∏: {_fmt_pct(dh)}")
        lines.append("")
        lines.append("–ò—Å—Ç–æ—á–Ω–∏–∫–∏: SQLite (grab_stats, gojek_stats), Open‚ÄëMeteo, Holidays cache")
        return "\n".join(lines)
    except Exception:
        return "8. üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –î–ù–ò (ML)\n" + ("‚Äî" * 72) + "\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–¥–µ–ª (–æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö)."


def _section9_recommendations(period: str, restaurant_id: int) -> str:
    try:
        # Use SHAP over the whole period to prioritize levers; exclude trivial features
        start_str, end_str = period.split("_")
        df = pd.read_csv("/workspace/data/merged_dataset.csv", parse_dates=["date"]) if os.path.exists("/workspace/data/merged_dataset.csv") else pd.DataFrame()
        sub = df[(df.get("restaurant_id") == restaurant_id) & (df.get("date") >= start_str) & (df.get("date") <= end_str)].copy() if not df.empty else pd.DataFrame()
        lines = []
        lines.append("9. üéØ –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        lines.append("‚Äî" * 72)
        if sub.empty:
            lines.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥.")
            return "\n".join(lines)

        # Load model and compute feature importances
        model, features, background = load_artifacts("/workspace/ml/artifacts")
        X = sub[features]
        pre = model.named_steps["pre"]
        mdl = model.named_steps["model"]
        X_pre = pre.transform(X)
        try:
            if background is not None and not background.empty:
                bg_pre = pre.transform(background[features])
                explainer = shap.TreeExplainer(mdl, data=bg_pre, feature_perturbation="interventional")
            else:
                explainer = shap.TreeExplainer(mdl, feature_perturbation="interventional")
            shap_values = explainer.shap_values(X_pre)
        except Exception:
            explainer = shap.TreeExplainer(mdl)
            shap_values = explainer.shap_values(X_pre)
        _, groups = _resolve_preprocessed_feature_groups(pre)
        import re as _re
        pat = [_re.compile(r"^orders_count(?!.*conversion).*"), _re.compile(r"^total_sales.*"), _re.compile(r"^restaurant_id$")]
        def is_excl(n: str) -> bool:
            return any(p.search(n) for p in pat)
        abs_sv = np.abs(shap_values)
        agg: Dict[str, float] = {}
        for feat, idxs in groups.items():
            if is_excl(feat) or not idxs:
                continue
            agg[feat] = float(abs_sv[:, idxs].mean())
        top = sorted(agg.items(), key=lambda x: x[1], reverse=True)[:8]

        # Group by categories
        cats: Dict[str, float] = {}
        for f, v in agg.items():
            c = _categorize_feature(f)
            cats[c] = cats.get(c, 0.0) + v
        tot = sum(cats.values()) or 1.0
        for k in list(cats.keys()):
            cats[k] = round(100.0 * cats[k] / tot, 1)

        lines.append("–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –ø–æ —Ñ–∞–∫—Ç–æ—Ä–∞–º (ML):")
        for f, v in top:
            lines.append(f"  ‚Ä¢ [{_categorize_feature(f)}] {_pretty_feature_name(f)}")
        lines.append("")
        lines.append("–í–∫–ª–∞–¥ –≥—Ä—É–ø–ø —Ñ–∞–∫—Ç–æ—Ä–æ–≤:")
        for k in ["Operations", "Marketing", "External", "Quality", "Other"]:
            if k in cats:
                lines.append(f"  ‚Ä¢ {k}: {cats[k]}%")
        lines.append("")
        lines.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:")
        if cats.get("Operations", 0) >= 30.0:
            lines.append("  ‚Ä¢ –°–æ–∫—Ä–∞—Ç–∏—Ç—å SLA (prep/accept/delivery) –≤ –ø–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞; –ø—Ä–µ–¥–∑–∞–≥–æ—Ç–æ–≤–∫–∏, —Å–ª–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–Ω—Ç—Ä–æ–ª—å –≤—ã–¥–∞—á–∏")
        if cats.get("Marketing", 0) >= 20.0:
            lines.append("  ‚Ä¢ –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±—é–¥–∂–µ—Ç –≤ —Å–≤—è–∑–∫–∏ —Å –ª—É—á—à–∏–º ROAS; —Ç–µ—Å—Ç –∫—Ä–µ–∞—Ç–∏–≤–æ–≤ –∏ –∞—É–¥–∏—Ç–æ—Ä–∏–π; –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Å—Ç–∞–≤–æ–∫")
        lines.append("  ‚Ä¢ –ü–æ–≥–æ–¥–Ω—ã–µ –ø—Ä–æ–º–æ –∏ –±–æ–Ω—É—Å—ã –∫—É—Ä—å–µ—Ä–∞–º –≤ –¥–æ–∂–¥—å; –ø–µ—Ä–µ–Ω–æ—Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π –Ω–∞ ¬´—Å—É—Ö–∏–µ¬ª –æ–∫–Ω–∞")
        lines.append("  ‚Ä¢ –£—á–∏—Ç—ã–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ (—Å–Ω–∏–∂–µ–Ω–∏–µ –±—é–¥–∂–µ—Ç–∞/–∞–∫—Ü–∏–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å)")
        lines.append("  ‚Ä¢ –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤: —Ä–∞–±–æ—Ç–∞ —Å –Ω–µ–≥–∞—Ç–∏–≤–æ–º, —É–ª—É—á—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–∂–∏–¥–∞–Ω–∏—è")
        return "\n".join(lines)
    except Exception:
        return "9. üéØ –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\n" + ("‚Äî" * 72) + "\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."


def generate_full_report(period: str, restaurant_id: int) -> str:
    basic = build_basic_report(period, restaurant_id)
    marketing = build_marketing_report(period, restaurant_id)
    quality = build_quality_report(period, restaurant_id)
    finance = basic.get("finance") if isinstance(basic, dict) else None

    parts = []
    # Dataset version banner
    banner = _dataset_version_banner()
    if banner:
        parts.append(banner)
        parts.append("")
    # Section 1
    parts.append(_section1_exec(basic))
    parts.append("")
    # Section 2
    parts.append(_section2_trends(basic))
    parts.append("")
    # Section 3
    parts.append(_section3_clients(period, restaurant_id))
    parts.append("")
    # Section 4
    parts.append(_section4_marketing(marketing))
    parts.append("")
    # Section 5
    if finance:
        parts.append(_section5_finance(finance))
        parts.append("")
    # Section 6
    parts.append(_section6_operations(period, restaurant_id))
    parts.append("")
    # Section 7
    parts.append(_section7_quality(quality))
    parts.append("")
    # Section 8 (ML Critical Days)
    parts.append(_section8_critical_days_ml(period, restaurant_id))
    parts.append("")
    # Section 9 (Recommendations)
    parts.append(_section9_recommendations(period, restaurant_id))
    parts.append("")
    return "\n".join(parts)