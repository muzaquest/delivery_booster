from __future__ import annotations

from typing import Optional, Dict
from datetime import date
import sqlite3
import pandas as pd
import os

from app.report_basic import (
    build_basic_report,
    build_marketing_report,
    build_quality_report,
)
from etl.data_loader import get_engine
import numpy as np
import re
from ml.inference import load_artifacts, _resolve_preprocessed_feature_groups
import shap
import json


def _fmt_idr(x: Optional[float]) -> str:
    if x is None:
        return "â€”"
    try:
        return f"{int(round(float(x))):,} IDR".replace(",", " ")
    except Exception:
        return str(x)


def _fmt_pct(x: Optional[float], digits: int = 1) -> str:
    if x is None:
        return "â€”"
    return f"{x:.{digits}f}%"


def _fmt_rate(x: Optional[float], digits: int = 2) -> str:
    if x is None:
        return "â€”"
    return f"{x:.{digits}f}"


def _hms_from_minutes(total_minutes: float) -> str:
    m = max(total_minutes, 0)
    hours = int(m // 60)
    minutes = int(m % 60)
    seconds = int(round((m - int(m)) * 60))
    return f"{hours}:{minutes:02d}:{seconds:02d}"


def _sec_to_minutes(avg_seconds: float) -> float:
    return float(avg_seconds) / 60.0


def _section1_exec(basic: Dict) -> str:
    es = basic["executive_summary"]
    grab = es["by_platform"]["grab"]
    gojek = es["by_platform"]["gojek"]

    total_rev = _fmt_idr(es["revenue_total"])
    grab_rev = _fmt_idr(grab.get("sales"))
    gojek_rev = _fmt_idr(gojek.get("sales"))

    total_orders = es.get("orders_total") or 0
    # Optional enriched fields
    fake = es.get("fake_orders", {})
    canc = es.get("cancellations", {})
    lost = es.get("lost_orders", {})
    succ = es.get("successful_orders", {})
    aov = es.get("aov", {})

    lines = []
    lines.append("ğŸ“Š 1. Ğ˜Ğ¡ĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ• Ğ Ğ•Ğ—Ğ®ĞœĞ•")
    lines.append("â€”" * 72)
    lines.append(f"ğŸ’° ĞĞ±Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°: {total_rev} (GRAB: {grab_rev} + GOJEK: {gojek_rev})")
    lines.append(f"ğŸ“¦ ĞĞ±Ñ‰Ğ¸Ğµ Ğ·Ğ°ĞºĞ°Ğ·Ñ‹: {total_orders}")
    lines.append(f"   â”œâ”€â”€ ğŸ“± GRAB: {int(grab.get('orders') or 0)} (ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾: {succ.get('grab','â€”')}, Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹: {canc.get('grab','â€”')}, Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸: {lost.get('grab','â€”')}, fake: {fake.get('grab','â€”')})")
    lines.append(f"   â””â”€â”€ ğŸ›µ GOJEK: {int(gojek.get('orders') or 0)} (ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾: {succ.get('gojek','â€”')}, Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹: {canc.get('gojek','â€”')}, Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸: {lost.get('gojek','â€”')}, fake: {fake.get('gojek','â€”')})")
    # AOVs
    if aov:
        lines.append(f"ğŸ’µ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ‡ĞµĞº (ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ): Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ { _fmt_idr(aov.get('total')) }; GRAB { _fmt_idr(aov.get('grab')) }; GOJEK { _fmt_idr(aov.get('gojek')) }")
    # Daily revenue
    drw = es.get('daily_revenue_workdays_avg')
    if drw is not None:
        lines.append(f"ğŸ“Š Ğ”Ğ½ĞµĞ²Ğ½Ğ°Ñ Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°: {_fmt_idr(drw)} (ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼ Ğ´Ğ½ÑĞ¼)")
    # Rating
    rat = es.get('rating_avg_total')
    if rat:
        lines.append(f"â­ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³: {_fmt_rate(float(rat), 2)}/5.0")
    # Clients
    cli = es.get('clients', {})
    if cli:
        tot = cli.get('total_unique')
        lines.append(f"ğŸ‘¥ ĞĞ±ÑĞ»ÑƒĞ¶ĞµĞ½Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²: {tot if tot is not None else 'â€”'}")
        g = cli.get('grab', {})
        j = cli.get('gojek', {})
        lines.append(f"   â”œâ”€â”€ ğŸ“± GRAB: {g.get('total','â€”')} (Ğ½Ğ¾Ğ²Ñ‹Ğµ: {g.get('new','â€”')}, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ: {g.get('repeated','â€”')}, Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ: {g.get('reactivated','â€”')})")
        lines.append(f"   â””â”€â”€ ğŸ›µ GOJEK: {j.get('new','â€”') + j.get('active','â€”') + j.get('returned','â€”') if all(isinstance(j.get(k), int) for k in ['new','active','returned']) else 'â€”'} (Ğ½Ğ¾Ğ²Ñ‹Ğµ: {j.get('new','â€”')}, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ: {j.get('active','â€”')}, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ¸Ğ²ÑˆĞ¸ĞµÑÑ: {j.get('returned','â€”')})")
        if tot is not None:
            lines.append(f"   ğŸ’¡ ĞĞ±Ñ‰Ğ¸Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚: {tot} ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²")
    # Marketing budget
    mb = es.get('marketing_budget', {})
    if mb:
        total_b = mb.get('total') or 0.0
        lines.append(f"ğŸ’¸ ĞœĞ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚: {_fmt_idr(total_b)} ({_fmt_pct(mb.get('share_of_revenue_pct'))} Ğ¾Ñ‚ Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ¸)")
        lines.append("ğŸ“Š Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚:")
        lines.append("   â”Œâ”€ ğŸ“± GRAB:")
        lines.append(f"   â”‚  ğŸ’° Ğ‘ÑĞ´Ğ¶ĞµÑ‚: {_fmt_idr(mb.get('grab'))}")
        # Additional ratios require per-platform revenue; already printed above implicitly; keep budget split concise
        lines.append("   â””â”€ ğŸ›µ GOJEK:")
        lines.append(f"      ğŸ’° Ğ‘ÑĞ´Ğ¶ĞµÑ‚: {_fmt_idr(mb.get('gojek'))}")
    # ROAS summary
    ro = es.get('roas', {})
    if ro:
        lines.append("")
        lines.append("ğŸ¯ ROAS ĞĞĞĞ›Ğ˜Ğ—:")
        lines.append(f"â”œâ”€â”€ ğŸ“± GRAB: {_fmt_rate(ro.get('grab'))}x")
        lines.append(f"â”œâ”€â”€ ğŸ›µ GOJEK: {_fmt_rate(ro.get('gojek'))}x")
        lines.append(f"â””â”€â”€ ğŸ¯ ĞĞ‘Ğ©Ğ˜Ğ™: {_fmt_rate(ro.get('total'))}x")
    return "\n".join(lines)


def _dataset_version_banner() -> str:
    try:
        metrics_path = "/workspace/ml/artifacts/metrics.json"
        if not os.path.exists(metrics_path):
            return ""
        with open(metrics_path, "r", encoding="utf-8") as f:
            m = json.load(f)
        h = str(m.get("dataset_hash",""))
        rows = m.get("dataset_rows")
        ts = m.get("run_at_utc")
        champ = m.get("champion")
        short = h[:10] if h else ""
        return f"Ğ’ĞµÑ€ÑĞ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: {short} Â· ÑÑ‚Ñ€Ğ¾Ğº: {rows} Â· Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»ÑÑ: {ts} Â· Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {champ}"
    except Exception:
        return ""


def _section2_trends(basic: Dict) -> str:
    st = basic["sales_trends"]
    monthly = st.get("monthly", {})
    lines = []
    lines.append("ğŸ“ˆ 2. ĞĞĞĞ›Ğ˜Ğ— ĞŸĞ ĞĞ”ĞĞ– Ğ˜ Ğ¢Ğ Ğ•ĞĞ”ĞĞ’")
    lines.append("â€”" * 72)
    lines.append("ğŸ“Š Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼:")
    for ym in sorted(monthly.keys()):
        m = monthly[ym]
        lines.append(
            f"  {ym}: {_fmt_idr(m['total_sales'])} ({m['days']} Ğ´Ğ½ĞµĞ¹, {_fmt_idr(m['avg_per_day'])}/Ğ´ĞµĞ½ÑŒ)"
        )
    w = st.get("weekend_vs_weekday", {})
    lines.append("")
    lines.append("ğŸ—“ï¸ Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ vs Ğ‘ÑƒĞ´Ğ½Ğ¸:")
    lines.append(f"  ğŸ“… Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ: {_fmt_idr(w.get('weekend_avg'))}")
    lines.append(f"  ğŸ“… Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ Ğ² Ğ±ÑƒĞ´Ğ½Ğ¸: {_fmt_idr(w.get('weekday_avg'))}")
    lines.append(f"  ğŸ“Š Ğ­Ñ„Ñ„ĞµĞºÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ…: {_fmt_pct(w.get('effect_pct'))}")

    best = st.get("best_day")
    worst = st.get("worst_day")
    if best:
        gp = best.get("by_platform", {})
        lines.append("ğŸ“Š ĞĞĞĞ›Ğ˜Ğ— Ğ ĞĞ‘ĞĞ§Ğ˜Ğ¥ Ğ”ĞĞ•Ğ™:")
        lines.append(
            f"ğŸ† Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ´ĞµĞ½ÑŒ: {best['date']} - {_fmt_idr(best['total_sales'])}"
        )
        lines.append(
            f"   ğŸ’° GRAB: {_fmt_idr(gp.get('grab',{}).get('sales'))} ({int(gp.get('grab',{}).get('orders') or 0)} Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ²) | "
            f"GOJEK: {_fmt_idr(gp.get('gojek',{}).get('sales'))} ({int(gp.get('gojek',{}).get('orders') or 0)} Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ²)"
        )
    if worst:
        gp = worst.get("by_platform", {})
        lines.append(
            f"ğŸ“‰ Ğ¥ÑƒĞ´ÑˆĞ¸Ğ¹ Ğ´ĞµĞ½ÑŒ: {worst['date']} - {_fmt_idr(worst['total_sales'])}"
        )
        lines.append(
            f"   ğŸ’° GRAB: {_fmt_idr(gp.get('grab',{}).get('sales'))} | GOJEK: {_fmt_idr(gp.get('gojek',{}).get('sales'))}"
        )
    return "\n".join(lines)


def _section4_marketing(mkt: Dict) -> str:
    f = mkt.get("funnel_grab", {})
    rm = mkt.get("roas_by_month", {})
    lines = []
    lines.append("ğŸ“ˆ 4. ĞœĞĞ ĞšĞ•Ğ¢Ğ˜ĞĞ“ĞĞ’ĞĞ¯ Ğ­Ğ¤Ğ¤Ğ•ĞšĞ¢Ğ˜Ğ’ĞĞĞ¡Ğ¢Ğ¬ Ğ˜ Ğ’ĞĞ ĞĞĞšĞ")
    lines.append("â€”" * 72)
    lines.append("ğŸ“Š ĞœĞ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ñ Ğ²Ğ¾Ñ€Ğ¾Ğ½ĞºĞ° (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ GRAB):")
    lines.append(f"  ğŸ‘ï¸ ĞŸĞ¾ĞºĞ°Ğ·Ñ‹ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹: {int(f.get('impressions') or 0)}")
    lines.append(
        f"  ğŸ”— ĞŸĞ¾ÑĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½Ñ: {int(f.get('menu_visits') or 0)} (CTR: {_fmt_pct((f.get('ctr') or 0)*100)})"
    )
    lines.append(
        f"  ğŸ›’ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ñ€Ğ·Ğ¸Ğ½Ñƒ: {int(f.get('add_to_cart') or 0)} (ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ: {_fmt_pct((f.get('conv_click_to_order') or 0)*100)} Ğ¾Ñ‚ ĞºĞ»Ğ¸ĞºĞ¾Ğ²)"
    )
    lines.append(
        f"  ğŸ“¦ Ğ—Ğ°ĞºĞ°Ğ·Ñ‹ Ğ¾Ñ‚ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹: {int(f.get('ads_orders') or 0)} (ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ: {_fmt_pct((f.get('conv_cart_to_order') or 0)*100)} Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ·Ğ¸Ğ½Ñ‹)"
    )
    lines.append("")
    lines.append("  ğŸ“Š ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• ĞšĞĞĞ’Ğ•Ğ Ğ¡Ğ˜Ğ˜:")
    lines.append(f"  â€¢ ğŸ¯ ĞŸĞ¾ĞºĞ°Ğ· â†’ Ğ—Ğ°ĞºĞ°Ğ·: {_fmt_pct((f.get('show_to_order') or 0)*100)}")
    lines.append(f"  â€¢ ğŸ”— ĞšĞ»Ğ¸Ğº â†’ Ğ—Ğ°ĞºĞ°Ğ·: {_fmt_pct((f.get('conv_click_to_order') or 0)*100)}")
    lines.append(f"  â€¢ ğŸ›’ ĞšĞ¾Ñ€Ğ·Ğ¸Ğ½Ğ° â†’ Ğ—Ğ°ĞºĞ°Ğ·: {_fmt_pct((f.get('conv_cart_to_order') or 0)*100)}")
    lines.append("")
    bouncers = int(max((f.get('menu_visits') or 0) - (f.get('add_to_cart') or 0), 0))
    aband = int(max((f.get('add_to_cart') or 0) - (f.get('ads_orders') or 0), 0))
    lines.append("  ğŸ“‰ Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ’ĞĞ ĞĞĞšĞ˜:")
    lines.append(
        f"  â€¢ ğŸ’” Ğ”Ğ¾Ğ»Ñ ÑƒÑˆĞµĞ´ÑˆĞ¸Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸: {_fmt_pct((f.get('bounce_rate') or 0)*100)} ({bouncers} ÑƒÑˆĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸)"
    )
    lines.append(
        f"  â€¢ ğŸ›’ Ğ”Ğ¾Ğ»Ñ Ğ½ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ·Ğ¸Ğ½: {_fmt_pct((f.get('abandoned_carts_rate') or 0)*100)} ({aband} Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸, Ğ½Ğ¾ Ğ½Ğµ ĞºÑƒĞ¿Ğ¸Ğ»Ğ¸)"
    )
    lines.append("")
    lines.append("  ğŸ’° ĞŸĞĞ¢Ğ•ĞĞ¦Ğ˜ĞĞ› ĞĞŸĞ¢Ğ˜ĞœĞ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ’ĞĞ ĞĞĞšĞ˜:")
    up = f.get("uplift_estimations", {})
    lines.append(
        f"  â€¢ ğŸ“ˆ Ğ¡Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ¸ ÑƒÑˆĞµĞ´ÑˆĞ¸Ñ… Ğ½Ğ° 10%: {_fmt_idr(up.get('reduce_bounce_10_pct_revenue'))}"
    )
    lines.append(
        f"  â€¢ ğŸ›’ Ğ£ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ·Ğ¸Ğ½: {_fmt_idr(up.get('eliminate_abandoned_revenue'))}"
    )
    lines.append(
        f"  â€¢ ğŸ¯ ĞĞ±Ñ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»: {_fmt_idr(up.get('total_uplift'))}"
    )
    lines.append("")
    lines.append("ğŸ’¸ Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ (GRAB):")
    lines.append(
        f"  ğŸ’° CPC: {_fmt_idr(f.get('cpc'))} (Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚: Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ã· Ğ¿Ğ¾ÑĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½Ñ)" 
    )
    lines.append(
        f"  ğŸ’° CPA: {_fmt_idr(f.get('cpa'))} (Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚: Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ã· Ğ·Ğ°ĞºĞ°Ğ·Ñ‹ Ğ¾Ñ‚ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹)"
    )
    return "\n".join(lines)


def _section5_finance(fin: Dict) -> str:
    lines = []
    lines.append("5. ğŸ’³ Ğ¤Ğ˜ĞĞĞĞ¡ĞĞ’Ğ«Ğ• ĞŸĞĞšĞĞ—ĞĞ¢Ğ•Ğ›Ğ˜")
    lines.append("â€”" * 72)
    payouts = fin.get("payouts", {})
    total_payouts = payouts.get("total") or 0.0
    grab_p = payouts.get("grab") or 0.0
    gojek_p = payouts.get("gojek") or 0.0
    grab_pct = (grab_p / total_payouts * 100.0) if total_payouts else None
    gojek_pct = (gojek_p / total_payouts * 100.0) if total_payouts else None
    lines.append("ğŸ’° Ğ’Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ñ‹:")
    lines.append(f"   â”œâ”€â”€ ğŸ“± GRAB: {_fmt_idr(grab_p)} ({_fmt_pct(grab_pct)})")
    lines.append(f"   â”œâ”€â”€ ğŸ›µ GOJEK: {_fmt_idr(gojek_p)} ({_fmt_pct(gojek_pct)})")
    lines.append(f"   â””â”€â”€ ğŸ’ ĞĞ±Ñ‰Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ñ‹: {_fmt_idr(total_payouts)}")

    ad_sales = fin.get("ad_sales")
    ad_share = (fin.get("ad_sales_share") or 0.0) * 100.0
    lines.append("ğŸ“Š Ğ ĞµĞºĞ»Ğ°Ğ¼Ğ½Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ:")
    lines.append(f"   â”œâ”€â”€ ğŸ’° ĞĞ±Ñ‰Ğ¸Ğµ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸: {_fmt_idr(ad_sales)}")
    lines.append(f"   â”œâ”€â”€ ğŸ“ˆ Ğ”Ğ¾Ğ»Ñ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶: {_fmt_pct(ad_share)}")
    lines.append(
        f"   â”œâ”€â”€ ğŸ¯ GRAB ROAS: {_fmt_rate(fin.get('roas',{}).get('grab'))}x"
    )
    lines.append(
        f"   â””â”€â”€ ğŸ¯ GOJEK ROAS: {_fmt_rate(fin.get('roas',{}).get('gojek'))}x"
    )

    tr = fin.get("take_rate", {})
    net_roas = fin.get("net_roas", {})
    lines.append("")
    lines.append("Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾:")
    lines.append(
        f"   â€¢ Take rate (Ğ´Ğ¾Ğ»Ñ ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ğ¹ Ğ¸ ÑƒĞ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¹): GRAB { _fmt_pct((tr.get('grab') or 0.0)*100) }, GOJEK { _fmt_pct((tr.get('gojek') or 0.0)*100) }"
    )
    lines.append(
        f"   â€¢ Ğ§Ğ¸ÑÑ‚Ñ‹Ğ¹ ROAS: GRAB {_fmt_rate(net_roas.get('grab'))}x; GOJEK {_fmt_rate(net_roas.get('gojek'))}x"
    )
    contrib = fin.get("contribution_per_ad_order_grab")
    if contrib is not None:
        lines.append(
            f"   â€¢ Ğ®Ğ½Ğ¸Ñ‚â€‘ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ° Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°ĞºĞ°Ğ·Ğ° (GRAB): {_fmt_idr(contrib)}"
        )
    return "\n".join(lines)


def _parse_time_to_minutes(val: str) -> Optional[float]:
    if val is None:
        return None
    s = str(val)
    parts = s.split(":")
    try:
        if len(parts) == 3:
            h, m, sec = parts
            return int(h) * 60 + int(m) + int(sec) / 60.0
        if len(parts) == 2:
            h, m = parts
            return int(h) * 60 + int(m)
        # fallback numeric
        return float(s)
    except Exception:
        return None


def _section6_operations(period: str, restaurant_id: int) -> str:
    eng = get_engine()
    start_str, end_str = period.split("_")
    start = pd.to_datetime(start_str)
    end = pd.to_datetime(end_str)

    # GRAB driver_waiting_time JSON average (seconds -> minutes if needed)
    qg = (
        "SELECT driver_waiting_time FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ? "
        "AND driver_waiting_time IS NOT NULL"
    )
    g = pd.read_sql_query(qg, eng, params=(restaurant_id, start_str, end_str))
    grab_wait_vals = []
    for v in g['driver_waiting_time'].dropna().tolist():
        try:
            if isinstance(v, str) and v.strip().startswith('{'):
                d = pd.read_json(pd.io.common.StringIO(v), typ='series') if False else None
                # Fallback manual parse
                import json as _json
                d = _json.loads(v)
                for k in ('avg','average','minutes','mean'):
                    if k in d:
                        val = float(d[k])
                        # heuristic: if seconds, convert to minutes
                        grab_wait_vals.append(val/60.0 if val > 60 else val)
                        break
            elif isinstance(v, (int, float)):
                val = float(v)
                grab_wait_vals.append(val/60.0 if val > 60 else val)
        except Exception:
            pass
    grab_wait_avg = sum(grab_wait_vals)/len(grab_wait_vals) if grab_wait_vals else None

    # GOJEK times
    qj = (
        "SELECT accepting_time, preparation_time, delivery_time, driver_waiting, close_time, stat_date "
        "FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?"
    )
    j = pd.read_sql_query(qj, eng, params=(restaurant_id, start_str, end_str))
    acc = pd.Series([_parse_time_to_minutes(x) for x in j['accepting_time'] if pd.notna(x)])
    prep = pd.Series([_parse_time_to_minutes(x) for x in j['preparation_time'] if pd.notna(x)])
    delv = pd.Series([_parse_time_to_minutes(x) for x in j['delivery_time'] if pd.notna(x)])
    drvw = pd.to_numeric(j['driver_waiting'], errors='coerce').dropna()

    # cancellations
    Cg = pd.read_sql_query(
        "SELECT SUM(cancelled_orders) c FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]['c'] or 0
    Cj_row = pd.read_sql_query(
        "SELECT SUM(cancelled_orders) c, SUM(lost_orders) l FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]
    Cj = Cj_row['c'] or 0; Lj = Cj_row['l'] or 0
    orders_total = (
        (pd.read_sql_query("SELECT SUM(orders) o FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?", eng, params=(restaurant_id, start_str, end_str)).iloc[0]['o'] or 0)
        + (pd.read_sql_query("SELECT SUM(orders) o FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?", eng, params=(restaurant_id, start_str, end_str)).iloc[0]['o'] or 0)
    )
    cancel_rate = ((Cg + Cj) / orders_total * 100.0) if orders_total else None

    # outages events > 1 hour
    events = []
    # GRAB offline_rate in minutes
    og = pd.read_sql_query(
        "SELECT stat_date, offline_rate FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ? AND offline_rate IS NOT NULL",
        eng, params=(restaurant_id, start_str, end_str)
    )
    for _, row in og.iterrows():
        mins = float(row['offline_rate'] or 0)
        if mins >= 60.0:
            events.append((pd.to_datetime(row['stat_date']).date(), 'GRAB', mins/60.0))
    # GOJEK close_time HH:MM:SS
    oj = pd.read_sql_query(
        "SELECT stat_date, close_time FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    )
    for _, row in oj.iterrows():
        ct = str(row['close_time']) if pd.notna(row['close_time']) else ''
        parts = ct.split(':')
        seconds = None
        try:
            if len(parts) == 3:
                h, m, s = parts
                seconds = int(h)*3600 + int(m)*60 + int(s)
        except Exception:
            seconds = None
        if seconds and seconds >= 3600:
            events.append((pd.to_datetime(row['stat_date']).date(), 'GOJEK', seconds/3600.0))

    # Potential losses
    # average hourly revenue by platform
    # platform sales
    sg = pd.read_sql_query(
        "SELECT SUM(sales) s FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]['s'] or 0.0
    sj = pd.read_sql_query(
        "SELECT SUM(sales) s FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?",
        eng, params=(restaurant_id, start_str, end_str)
    ).iloc[0]['s'] or 0.0
    num_days = (end - start).days + 1
    hr_g = (sg / (num_days*24.0)) if num_days>0 else 0.0
    hr_j = (sj / (num_days*24.0)) if num_days>0 else 0.0

    total_loss_g = sum((hrs*hr_g) for (d,plat,hrs) in events if plat=='GRAB')
    total_loss_j = sum((hrs*hr_j) for (d,plat,hrs) in events if plat=='GOJEK')

    lines = []
    lines.append("6. â° ĞĞŸĞ•Ğ ĞĞ¦Ğ˜ĞĞĞĞ«Ğ• ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜")
    lines.append("â€”" * 72)
    lines.append("ğŸŸ¢ GRAB:")
    lines.append(f"â””â”€â”€ â° Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹: {_fmt_rate(grab_wait_avg,1)} Ğ¼Ğ¸Ğ½")
    lines.append("")
    lines.append("ğŸŸ  GOJEK:")
    lines.append(f"â”œâ”€â”€ â±ï¸ Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ: {_fmt_rate(prep.mean() if not prep.empty else None,1)} Ğ¼Ğ¸Ğ½")
    lines.append(f"â”œâ”€â”€ ğŸš— Ğ’Ñ€ĞµĞ¼Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸: {_fmt_rate(delv.mean() if not delv.empty else None,1)} Ğ¼Ğ¸Ğ½  ")
    lines.append(f"â””â”€â”€ â° Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹: {_fmt_rate(drvw.mean() if not drvw.empty else None,1)} Ğ¼Ğ¸Ğ½")
    lines.append("")
    lines.append("âš ï¸ ĞĞŸĞ•Ğ ĞĞ¦Ğ˜ĞĞĞĞĞ¯ Ğ­Ğ¤Ğ¤Ğ•ĞšĞ¢Ğ˜Ğ’ĞĞĞ¡Ğ¢Ğ¬")
    lines.append("â€”" * 72)
    lines.append("ğŸš« ĞÑ‚Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ°Ğ·Ñ‹:")
    lines.append(f"   â”œâ”€â”€ ğŸ“± GRAB: {int(Cg)} Ğ·Ğ°ĞºĞ°Ğ·Ğ°")
    lines.append(f"   â””â”€â”€ ğŸ›µ GOJEK: {int(Cj)} Ğ·Ğ°ĞºĞ°Ğ·Ğ°")
    lines.append(f"   ğŸ’¡ Ğ’ÑĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ…: {int(Cg+Cj)} Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ² ({_fmt_pct(cancel_rate)})")
    lines.append("")
    if events:
        total_loss = total_loss_g + total_loss_j
        total_sales = sg + sj
        loss_pct = (total_loss/total_sales*100.0) if total_sales else None
        lines.append("ğŸ”§ ĞĞŸĞ•Ğ ĞĞ¦Ğ˜ĞĞĞĞ«Ğ• Ğ¡Ğ‘ĞĞ˜ ĞŸĞ›ĞĞ¢Ğ¤ĞĞ Ğœ:")
        # aggregate durations per platform
        dur_g = sum(hrs for (_,plat,hrs) in events if plat=='GRAB')
        dur_j = sum(hrs for (_,plat,hrs) in events if plat=='GOJEK')
        from datetime import timedelta
        def hms_from_hours(h):
            h_int = int(h)
            m = int((h - h_int)*60)
            s = int(round(((h - h_int)*60 - m)*60))
            return f"{h_int}:{m:02d}:{s:02d}"
        lines.append(f"â”œâ”€â”€ ğŸ“± GRAB: {len([1 for _,p,_ in events if p=='GRAB'])} ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ½Ñ ({hms_from_hours(dur_g)} Ğ¾Ğ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ)")
        lines.append(f"â”œâ”€â”€ ğŸ›µ GOJEK: {len([1 for _,p,_ in events if p=='GOJEK'])} ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ½Ñ ({hms_from_hours(dur_j)} Ğ¾Ğ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ)")
        lines.append(f"â””â”€â”€ ğŸ’¸ ĞŸĞ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸: {_fmt_idr(total_loss)} ({_fmt_pct(loss_pct)})")
        if events:
            lines.append("")
            lines.append("ğŸš¨ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ¡Ğ‘ĞĞ˜ (>1 Ñ‡Ğ°ÑĞ°):")
            # sort by date
            for d, plat, hrs in sorted(events, key=lambda x: x[0]):
                loss = hrs*(hr_g if plat=='GRAB' else hr_j)
                lines.append(f"   â€¢ {d}: {plat} offline {hms_from_hours(hrs)} (Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸: ~{_fmt_idr(loss)})")
    return "\n".join(lines)


def _section3_clients(period: str, restaurant_id: int) -> str:
    eng = get_engine()
    start_str, end_str = period.split("_")
    qg = (
        "SELECT SUM(new_customers) new, SUM(repeated_customers) rep, SUM(reactivated_customers) rea, SUM(total_customers) tot, "
        "SUM(earned_new_customers) enew, SUM(earned_repeated_customers) erep, SUM(earned_reactivated_customers) erea "
        "FROM grab_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?"
    )
    qj = (
        "SELECT SUM(new_client) new, SUM(active_client) act, SUM(returned_client) ret "
        "FROM gojek_stats WHERE restaurant_id=? AND stat_date BETWEEN ? AND ?"
    )
    g = pd.read_sql_query(qg, eng, params=(restaurant_id, start_str, end_str)).iloc[0].fillna(0)
    j = pd.read_sql_query(qj, eng, params=(restaurant_id, start_str, end_str)).iloc[0].fillna(0)
    grab_new, grab_rep, grab_rea, grab_tot = int(g['new']), int(g['rep']), int(g['rea']), int(g['tot'])
    gojek_new, gojek_act, gojek_ret = int(j['new']), int(j['act']), int(j['ret'])
    total_unique = grab_tot + gojek_new + gojek_act + gojek_ret  # Ğ²ĞµÑ€Ñ…Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ°

    lines = []
    lines.append("ğŸ‘¥ 3. Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— ĞšĞ›Ğ˜Ğ•ĞĞ¢Ğ¡ĞšĞĞ™ Ğ‘ĞĞ—Ğ«")
    lines.append("â€”" * 72)
    lines.append("ğŸ“Š Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚ÑĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ (GRAB + GOJEK):")
    lines.append(f"  ğŸ†• ĞĞ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹: {grab_new + gojek_new}")
    lines.append(f"    ğŸ“± GRAB: {grab_new} | ğŸ›µ GOJEK: {gojek_new}")
    lines.append(f"  ğŸ”„ ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹: {grab_rep + gojek_act}")
    lines.append(f"    ğŸ“± GRAB: {grab_rep} | ğŸ›µ GOJEK: {gojek_act}")
    lines.append(f"  ğŸ“² Ğ ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ: {grab_rea + gojek_ret}")
    lines.append(f"    ğŸ“± GRAB: {grab_rea} | ğŸ›µ GOJEK: {gojek_ret}")
    lines.append("")
    lines.append("ğŸ’° Ğ”Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ GRAB, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹):")
    lines.append(f"  ğŸ†• ĞĞ¾Ğ²Ñ‹Ğµ: {_fmt_idr(g['enew'])}")
    lines.append(f"  ğŸ”„ ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ: {_fmt_idr(g['erep'])}")
    lines.append(f"  ğŸ“² Ğ ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ: {_fmt_idr(g['erea'])}")
    return "\n".join(lines)


def _section7_quality(quality: Dict) -> str:
    r = quality.get("ratings", {})
    lines = []
    lines.append("7. â­ ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ ĞĞ‘Ğ¡Ğ›Ğ£Ğ–Ğ˜Ğ’ĞĞĞ˜Ğ¯ Ğ˜ Ğ£Ğ”ĞĞ’Ğ›Ğ•Ğ¢Ğ’ĞĞ Ğ•ĞĞĞĞ¡Ğ¢Ğ¬ (GOJEK)")
    lines.append("â€”" * 72)
    total = r.get("total") or 0
    lines.append(f"ğŸ“Š Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº (Ğ²ÑĞµĞ³Ğ¾: {total}):")
    lines.append(f"  â­â­â­â­â­ 5 Ğ·Ğ²ĞµĞ·Ğ´: {r.get('five',0)} ({_fmt_pct((r.get('five',0)/total*100) if total else None)})")
    lines.append(f"  â­â­â­â­ 4 Ğ·Ğ²ĞµĞ·Ğ´Ñ‹: {r.get('four',0)} ({_fmt_pct((r.get('four',0)/total*100) if total else None)})")
    lines.append(f"  â­â­â­ 3 Ğ·Ğ²ĞµĞ·Ğ´Ñ‹: {r.get('three',0)} ({_fmt_pct((r.get('three',0)/total*100) if total else None)})")
    lines.append(f"  â­â­ 2 Ğ·Ğ²ĞµĞ·Ğ´Ñ‹: {r.get('two',0)} ({_fmt_pct((r.get('two',0)/total*100) if total else None)})")
    lines.append(f"  â­ 1 Ğ·Ğ²ĞµĞ·Ğ´Ğ°: {r.get('one',0)} ({_fmt_pct((r.get('one',0)/total*100) if total else None)})")
    lines.append("")
    lines.append(f"ğŸ“ˆ Ğ˜Ğ½Ğ´ĞµĞºÑ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: {_fmt_rate(r.get('satisfaction_index'))}/5.0")
    lines.append(f"ğŸš¨ ĞĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ (1-2â˜…): {r.get('negative_1_2',{}).get('count',0)} ({_fmt_pct(r.get('negative_1_2',{}).get('percent'))})")
    lines.append("")
    lines.append("ğŸ“Š Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¿Ğ»Ğ¾Ñ…Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº (Ğ½Ğµ 5â˜…):")
    lines.append(f"  ğŸ“ˆ ĞŸĞ»Ğ¾Ñ…Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²ÑĞµĞ³Ğ¾: {r.get('not_five',{}).get('count',0)} Ğ¸Ğ· {total} ({_fmt_pct(r.get('not_five',{}).get('percent'))})")
    lines.append(f"  ğŸ“¦ Ğ£ÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ² GOJEK Ğ½Ğ° 1 Ğ¿Ğ»Ğ¾Ñ…ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ: {_fmt_rate(quality.get('orders_per_not_five_rating'))}")
    return "\n".join(lines)


def _fmt_minutes_to_hhmmss(mins: Optional[float]) -> str:
    if mins is None or (isinstance(mins, float) and np.isnan(mins)):
        return "â€”"
    try:
        total_seconds = int(round(float(mins) * 60))
        h = total_seconds // 3600
        m = (total_seconds % 3600) // 60
        s = total_seconds % 60
        return f"{h}:{m:02d}:{s:02d}"
    except Exception:
        return "â€”"


def _categorize_feature(name: str) -> str:
    n = name.lower()
    if n.startswith("mkt_") or "ads_spend" in n or "impressions" in n or "roas" in n:
        return "Marketing"
    if n.startswith("ops_") or any(k in n for k in ["accepting_time", "preparation_time", "delivery_time", "outage_", "offline_"]):
        return "Operations"
    if n in ("temp", "rain", "wind", "humidity", "tourist_flow", "is_holiday", "day_of_week", "is_weekend") or any(k in n for k in ["temp_", "rain_", "wind_", "humidity_", "tourist_flow_"]):
        return "External"
    if "rating" in n:
        return "Quality"
    return "Other"


def _section8_critical_days_ml(period: str, restaurant_id: int) -> str:
    try:
        start_str, end_str = period.split("_")
        df = pd.read_csv("/workspace/data/merged_dataset.csv", parse_dates=["date"])  # daily rows per restaurant
        sub = df[(df["restaurant_id"] == restaurant_id) & (df["date"] >= start_str) & (df["date"] <= end_str)].copy()
        if sub.empty:
            return "8. ğŸš¨ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ”ĞĞ˜ (ML)\n" + ("â€”" * 72) + "\nĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´."

        # Median per day and critical threshold (â‰¤ -30% Ğº Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğµ)
        daily = sub.groupby("date", as_index=False)["total_sales"].sum().sort_values("date")
        med = float(daily["total_sales"].median()) if len(daily) else 0.0
        thr = 0.7 * med
        critical_dates = daily.loc[daily["total_sales"] <= thr, "date"].dt.normalize().tolist()

        lines: list[str] = []
        lines.append("8. ğŸš¨ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ”ĞĞ˜ (ML)")
        lines.append("â€”" * 72)
        if not critical_dates:
            lines.append("Ğ’ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğµ Ğ½ĞµÑ‚ Ğ´Ğ½ĞµĞ¹ Ñ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸ĞµĞ¼ â‰¥ 30% Ğº Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğµ.")
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ¼ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ÑÑ€ĞµĞ· Ğ¿Ğ¾ Ğ´Ğ¾Ğ¶Ğ´Ñ/Ğ¿Ñ€Ğ°Ğ·Ğ´Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ°
            sub['heavy_rain'] = (sub['rain'].fillna(0.0) >= 10.0).astype(int)
            def _mean(series):
                s = pd.to_numeric(series, errors='coerce')
                return float(s.mean()) if len(s) else 0.0
            by_rain = sub.groupby('heavy_rain')['total_sales'].mean().to_dict()
            if 0 in by_rain:
                dr = (by_rain.get(1, by_rain[0]) - by_rain[0]) / (by_rain[0] or 1.0) * 100.0
                lines.append(f"ğŸŒ§ï¸ Ğ­Ñ„Ñ„ĞµĞºÑ‚ Ğ´Ğ¾Ğ¶Ğ´Ñ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ…): {_fmt_pct(dr)}")
            by_h = sub.groupby(sub['is_holiday'].fillna(0).astype(int))['total_sales'].mean().to_dict()
            if 0 in by_h:
                dh = (by_h.get(1, by_h[0]) - by_h[0]) / (by_h[0] or 1.0) * 100.0
                lines.append(f"ğŸŒ Ğ­Ñ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€Ğ°Ğ·Ğ´Ğ½Ğ¸ĞºĞ¾Ğ² (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ…): {_fmt_pct(dh)}")
            return "\n".join(lines)

        # Prepare SHAP per-row
        model, features, background = load_artifacts()
        X = sub[features]
        pre = model.named_steps["pre"]
        mdl = model.named_steps["model"]
        X_pre = pre.transform(X)
        try:
            if background is not None and not background.empty:
                bg_pre = pre.transform(background[features])
                explainer = shap.TreeExplainer(mdl, data=bg_pre, feature_perturbation="interventional")
            else:
                explainer = shap.TreeExplainer(mdl, feature_perturbation="interventional")
            shap_values = explainer.shap_values(X_pre)
        except Exception:
            explainer = shap.TreeExplainer(mdl)
            shap_values = explainer.shap_values(X_pre)

        _, groups = _resolve_preprocessed_feature_groups(pre)
        # Exclude trivial features
        pat = [re.compile(r"^orders_count(?!.*conversion).*"), re.compile(r"^total_sales.*"), re.compile(r"^restaurant_id$")]
        def is_excluded(n: str) -> bool:
            return any(p.search(n) for p in pat)

        eng = get_engine()
        for d in critical_dates:
            day_mask = sub["date"].dt.normalize() == d
            idxs = np.where(day_mask.values)[0]
            if len(idxs) == 0:
                continue
            # Aggregate contributions over all rows of that date (should typically be 1 per date)
            contrib_sum: Dict[str, float] = {}
            for i in idxs:
                for feat, cols in groups.items():
                    if is_excluded(feat):
                        continue
                    if not cols:
                        continue
                    val = float(np.sum(shap_values[i, cols]))
                    contrib_sum[feat] = contrib_sum.get(feat, 0.0) + val
            # Top-10 by |impact|
            top10 = sorted(contrib_sum.items(), key=lambda x: abs(x[1]), reverse=True)[:10]
            total_abs = sum(abs(v) for v in contrib_sum.values()) or 1.0

            # Group shares
            group_shares: Dict[str, float] = {}
            for feat, val in contrib_sum.items():
                cat = _categorize_feature(feat)
                group_shares[cat] = group_shares.get(cat, 0.0) + abs(val)
            for k in list(group_shares.keys()):
                group_shares[k] = round(100.0 * group_shares[k] / total_abs, 1)

            # Day-level raw details from stats
            ds = str(d.date())
            qg = pd.read_sql_query(
                "SELECT sales, orders, ads_spend, ads_sales, offline_rate FROM grab_stats WHERE restaurant_id=? AND stat_date=?",
                eng, params=(restaurant_id, ds)
            )
            qj = pd.read_sql_query(
                "SELECT sales, orders, ads_spend, ads_sales, accepting_time, preparation_time, delivery_time, close_time FROM gojek_stats WHERE restaurant_id=? AND stat_date=?",
                eng, params=(restaurant_id, ds)
            )
            grab_off_mins = float(qg.iloc[0]["offline_rate"]) if (not qg.empty and pd.notna(qg.iloc[0]["offline_rate"])) else None
            gojek_close = str(qj.iloc[0]["close_time"]) if (not qj.empty and pd.notna(qj.iloc[0]["close_time"])) else ""
            # close_time may be HH:MM:SS
            def _hms_close(s: str) -> str:
                parts = s.split(":") if s else []
                try:
                    if len(parts) == 3:
                        h, m, sec = parts
                        return f"{int(h)}:{int(m):02d}:{int(sec):02d}"
                except Exception:
                    pass
                return "â€”"

            # Weather/holiday from dataset row (first match of the date)
            row = sub.loc[day_mask].iloc[0]
            rain = float(row.get("rain")) if pd.notna(row.get("rain")) else None
            temp = float(row.get("temp")) if pd.notna(row.get("temp")) else None
            wind = float(row.get("wind")) if pd.notna(row.get("wind")) else None
            hum = float(row.get("humidity")) if pd.notna(row.get("humidity")) else None
            is_hol = int(row.get("is_holiday")) if pd.notna(row.get("is_holiday")) else 0
            total_sales_day = float(daily.loc[daily["date"] == d, "total_sales"].iloc[0])
            delta_pct = ((total_sales_day - med) / med * 100.0) if med else None

            lines.append(f"ğŸ“‰ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ Ğ”Ğ•ĞĞ¬: {ds} (Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°: {_fmt_idr(total_sales_day)}; Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğµ: {_fmt_pct(delta_pct)})")
            lines.append("â€”" * 72)
            # Factors table (concise)
            lines.append("ğŸ” Ğ¢ĞĞŸâ€‘Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ (ML):")
            for feat, val in top10:
                cat = _categorize_feature(feat)
                direction = "â†‘" if val > 0 else "â†“"
                share = round(100.0 * abs(val) / total_abs, 1)
                lines.append(f"  â€¢ [{cat}] {feat}: {direction} Ğ²ĞºĞ»Ğ°Ğ´ ~{_fmt_idr(abs(val))} ({share}%)")
            lines.append("")
            lines.append("ğŸ“Š Ğ’ĞºĞ»Ğ°Ğ´ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²:")
            for cat in ["Operations", "Marketing", "External", "Quality", "Other"]:
                if cat in group_shares:
                    lines.append(f"  â€¢ {cat}: {group_shares[cat]}%")
            lines.append("")
            lines.append("ğŸ“… ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ½Ñ:")
            # Platforms/offline
            lines.append(f"  â€¢ ğŸ“± GRAB Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½: {_fmt_minutes_to_hhmmss(grab_off_mins)}")
            lines.append(f"  â€¢ ğŸ›µ GOJEK Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½: {_hms_close(gojek_close)}")
            # Marketing
            if not qg.empty:
                gs = qg.iloc[0]
                roas_g = (float(gs["ads_sales"]) / float(gs["ads_spend"])) if (pd.notna(gs["ads_spend"]) and float(gs["ads_spend"])>0) else None
                lines.append(f"  â€¢ ğŸ¯ GRAB: spend {_fmt_idr(gs['ads_spend'])}, ROAS {_fmt_rate(roas_g)}x")
            if not qj.empty:
                js = qj.iloc[0]
                roas_j = (float(js["ads_sales"]) / float(js["ads_spend"])) if (pd.notna(js["ads_spend"]) and float(js["ads_spend"])>0) else None
                lines.append(f"  â€¢ ğŸ¯ GOJEK: spend {_fmt_idr(js['ads_spend'])}, ROAS {_fmt_rate(roas_j)}x")
            # Operations (GOJEK times)
            if not qj.empty:
                def _to_min(v):
                    s = str(v)
                    parts = s.split(":")
                    try:
                        if len(parts) == 3:
                            h, m, sec = parts
                            return int(h)*60 + int(m) + int(sec)/60.0
                    except Exception:
                        return None
                    try:
                        return float(s)
                    except Exception:
                        return None
                lines.append(f"  â€¢ â±ï¸ ĞŸÑ€Ğ¸Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: {_fmt_rate(_to_min(qj.iloc[0].get('preparation_time')))} Ğ¼Ğ¸Ğ½")
                lines.append(f"  â€¢ â³ ĞŸĞ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ: {_fmt_rate(_to_min(qj.iloc[0].get('accepting_time')))} Ğ¼Ğ¸Ğ½")
                lines.append(f"  â€¢ ğŸš— Ğ”Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ°: {_fmt_rate(_to_min(qj.iloc[0].get('delivery_time')))} Ğ¼Ğ¸Ğ½")
            # Weather/holiday
            lines.append(f"  â€¢ ğŸŒ§ï¸ Ğ”Ğ¾Ğ¶Ğ´ÑŒ: {rain if rain is not None else 'â€”'} Ğ¼Ğ¼; ğŸŒ¡ï¸ Ğ¢ĞµĞ¼Ğ¿.: {temp if temp is not None else 'â€”'}Â°C; ğŸŒ¬ï¸ Ğ’ĞµÑ‚ĞµÑ€: {wind if wind is not None else 'â€”'}; ğŸ’§Ğ’Ğ»Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ: {hum if hum is not None else 'â€”'}")
            lines.append(f"  â€¢ ğŸŒ ĞŸÑ€Ğ°Ğ·Ğ´Ğ½Ğ¸Ğº: {'Ğ´Ğ°' if is_hol else 'Ğ½ĞµÑ‚'}")
            lines.append("")
            # What-if: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ SLA Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ°, ÑĞ½ÑÑ‚Ğ¸Ğµ Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½Ğ°
            try:
                row_idx = idxs[0]
                xrow = X.iloc[[row_idx]].copy(deep=True)
                for col in ["preparation_time_mean", "accepting_time_mean", "delivery_time_mean"]:
                    if col in xrow.columns and pd.notna(xrow.iloc[0][col]):
                        xrow.loc[xrow.index[0], col] = max(0.0, float(xrow.iloc[0][col]) * 0.9)
                if "outage_offline_rate_grab" in xrow.columns and pd.notna(xrow.iloc[0]["outage_offline_rate_grab"]):
                    xrow.loc[xrow.index[0], "outage_offline_rate_grab"] = 0.0
                if "ads_spend_total" in xrow.columns and pd.notna(xrow.iloc[0]["ads_spend_total"]):
                    xrow.loc[xrow.index[0], "ads_spend_total"] = float(xrow.iloc[0]["ads_spend_total"]) * 1.1
                uplift = float(model.predict(xrow)[0] - model.predict(X.iloc[[row_idx]])[0])
                lines.append(f"ğŸ”® Whatâ€‘if (âˆ’10% SLA, +10% Ğ±ÑĞ´Ğ¶ĞµÑ‚, Ğ±ĞµĞ· Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½Ğ°): Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ ~{_fmt_idr(uplift)}")
            except Exception:
                pass
            lines.append("")

        # Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ°: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ´Ğ¾Ğ¶Ğ´Ñ Ğ¸ Ğ¿Ñ€Ğ°Ğ·Ğ´Ğ½Ğ¸ĞºĞ¾Ğ²
        lines.append("Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´:")
        sub['heavy_rain'] = (sub['rain'].fillna(0.0) >= 10.0).astype(int)
        by_rain = sub.groupby('heavy_rain')['total_sales'].mean().to_dict()
        if 0 in by_rain:
            dr = (by_rain.get(1, by_rain[0]) - by_rain[0]) / (by_rain[0] or 1.0) * 100.0
            lines.append(f"  â€¢ ğŸŒ§ï¸ Ğ”Ğ¾Ğ¶Ğ´ÑŒ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ…): {_fmt_pct(dr)}")
        by_h = sub.groupby(sub['is_holiday'].fillna(0).astype(int))['total_sales'].mean().to_dict()
        if 0 in by_h:
            dh = (by_h.get(1, by_h[0]) - by_h[0]) / (by_h[0] or 1.0) * 100.0
            lines.append(f"  â€¢ ğŸŒ ĞŸÑ€Ğ°Ğ·Ğ´Ğ½Ğ¸ĞºĞ¸ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ…): {_fmt_pct(dh)}")
        lines.append("")
        lines.append("Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: SQLite (grab_stats, gojek_stats), Openâ€‘Meteo, Holidays cache")
        return "\n".join(lines)
    except Exception:
        return "8. ğŸš¨ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ”ĞĞ˜ (ML)\n" + ("â€”" * 72) + "\nĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ» (Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)."


def _section9_recommendations(period: str, restaurant_id: int) -> str:
    try:
        # Use SHAP over the whole period to prioritize levers; exclude trivial features
        start_str, end_str = period.split("_")
        df = pd.read_csv("/workspace/data/merged_dataset.csv", parse_dates=["date"]) if os.path.exists("/workspace/data/merged_dataset.csv") else pd.DataFrame()
        sub = df[(df.get("restaurant_id") == restaurant_id) & (df.get("date") >= start_str) & (df.get("date") <= end_str)].copy() if not df.empty else pd.DataFrame()
        lines = []
        lines.append("9. ğŸ¯ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜")
        lines.append("â€”" * 72)
        if sub.empty:
            lines.append("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´.")
            return "\n".join(lines)

        # Load model and compute feature importances
        model, features, background = load_artifacts("/workspace/ml/artifacts")
        X = sub[features]
        pre = model.named_steps["pre"]
        mdl = model.named_steps["model"]
        X_pre = pre.transform(X)
        try:
            if background is not None and not background.empty:
                bg_pre = pre.transform(background[features])
                explainer = shap.TreeExplainer(mdl, data=bg_pre, feature_perturbation="interventional")
            else:
                explainer = shap.TreeExplainer(mdl, feature_perturbation="interventional")
            shap_values = explainer.shap_values(X_pre)
        except Exception:
            explainer = shap.TreeExplainer(mdl)
            shap_values = explainer.shap_values(X_pre)
        _, groups = _resolve_preprocessed_feature_groups(pre)
        import re as _re
        pat = [_re.compile(r"^orders_count(?!.*conversion).*"), _re.compile(r"^total_sales.*"), _re.compile(r"^restaurant_id$")]
        def is_excl(n: str) -> bool:
            return any(p.search(n) for p in pat)
        abs_sv = np.abs(shap_values)
        agg: Dict[str, float] = {}
        for feat, idxs in groups.items():
            if is_excl(feat) or not idxs:
                continue
            agg[feat] = float(abs_sv[:, idxs].mean())
        top = sorted(agg.items(), key=lambda x: x[1], reverse=True)[:8]

        # Group by categories
        cats: Dict[str, float] = {}
        for f, v in agg.items():
            c = _categorize_feature(f)
            cats[c] = cats.get(c, 0.0) + v
        tot = sum(cats.values()) or 1.0
        for k in list(cats.keys()):
            cats[k] = round(100.0 * cats[k] / tot, 1)

        lines.append("ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñ‹ Ğ¿Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼ (ML):")
        for f, v in top:
            lines.append(f"  â€¢ [{_categorize_feature(f)}] {f}")
        lines.append("")
        lines.append("Ğ’ĞºĞ»Ğ°Ğ´ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²:")
        for k in ["Operations", "Marketing", "External", "Quality", "Other"]:
            if k in cats:
                lines.append(f"  â€¢ {k}: {cats[k]}%")
        lines.append("")
        lines.append("Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ:")
        if cats.get("Operations", 0) >= 30.0:
            lines.append("  â€¢ Ğ¡Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ SLA (prep/accept/delivery) Ğ² Ğ¿Ğ¸ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¾ĞºĞ½Ğ°; Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸, ÑĞ»Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹Ğ´Ğ°Ñ‡Ğ¸")
        if cats.get("Marketing", 0) >= 20.0:
            lines.append("  â€¢ ĞŸĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ² ÑĞ²ÑĞ·ĞºĞ¸ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ ROAS; Ñ‚ĞµÑÑ‚ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹; ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑÑ‚Ğ°Ğ²Ğ¾Ğº")
        lines.append("  â€¢ ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ Ğ¸ Ğ±Ğ¾Ğ½ÑƒÑÑ‹ ĞºÑƒÑ€ÑŒĞµÑ€Ğ°Ğ¼ Ğ² Ğ´Ğ¾Ğ¶Ğ´ÑŒ; Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Â«ÑÑƒÑ…Ğ¸ĞµÂ» Ğ¾ĞºĞ½Ğ°")
        lines.append("  â€¢ Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ·Ğ´Ğ½Ğ¸ĞºĞ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ (ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°/Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ)")
        lines.append("  â€¢ ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²: Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ")
        return "\n".join(lines)
    except Exception:
        return "9. ğŸ¯ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜\n" + ("â€”" * 72) + "\nĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸."


def generate_full_report(period: str, restaurant_id: int) -> str:
    basic = build_basic_report(period, restaurant_id)
    marketing = build_marketing_report(period, restaurant_id)
    quality = build_quality_report(period, restaurant_id)
    finance = basic.get("finance") if isinstance(basic, dict) else None

    parts = []
    # Dataset version banner
    banner = _dataset_version_banner()
    if banner:
        parts.append(banner)
        parts.append("")
    # Section 1
    parts.append(_section1_exec(basic))
    parts.append("")
    # Section 2
    parts.append(_section2_trends(basic))
    parts.append("")
    # Section 3
    parts.append(_section3_clients(period, restaurant_id))
    parts.append("")
    # Section 4
    parts.append(_section4_marketing(marketing))
    parts.append("")
    # Section 5
    if finance:
        parts.append(_section5_finance(finance))
        parts.append("")
    # Section 6
    parts.append(_section6_operations(period, restaurant_id))
    parts.append("")
    # Section 7
    parts.append(_section7_quality(quality))
    parts.append("")
    # Section 8 (ML Critical Days)
    parts.append(_section8_critical_days_ml(period, restaurant_id))
    parts.append("")
    # Section 9 (Recommendations)
    parts.append(_section9_recommendations(period, restaurant_id))
    parts.append("")
    return "\n".join(parts)